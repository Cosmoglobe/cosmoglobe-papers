\documentclass[twocolumn]{../common/aa}
%\documentclass[referee]{aa}

\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{txfonts}
\usepackage{color}
\usepackage{natbib}
\usepackage{float}
%\usepackage{stfloats}
\usepackage{dblfloatfix}
\usepackage{afterpage}
\usepackage{ifthen}
\usepackage[morefloats=12]{morefloats}
\usepackage{placeins}
\usepackage{multicol}
\bibpunct{(}{)}{;}{a}{}{,}
\usepackage[switch]{lineno}
\definecolor{linkcolor}{rgb}{0.6,0,0}
\definecolor{citecolor}{rgb}{0,0,0.75}
\definecolor{urlcolor}{rgb}{0.12,0.46,0.7}
\usepackage[breaklinks, colorlinks, urlcolor=urlcolor,
    linkcolor=linkcolor,citecolor=citecolor,pdfencoding=auto]{hyperref}
\hypersetup{linktocpage}
\usepackage{bold-extra}
\usepackage{xcolor}

%\usepackage[grid,
%  gridcolor=red!20,
%  subgridcolor=green!20,
%  gridunit=cm]{eso-pic}
%

\input{../common/Planck}

\def\WMAP{\emph{WMAP}}
\def\WMAPnine{\emph{WMAP9}}
\def\COBE{\emph{COBE}}
\def\wmap{\emph{WMAP}}
\def\planck{\emph{Planck}}
\def\Planck{\emph{Planck}}
\def\LCDM{$\Lambda$CDM}
\def\ffp{FFP6}
\def\unionmask{U73}
\def\nside{N_{\mathrm{side}}}

\def\healpix{\texttt{HEALPix}}
\def\commander{\texttt{Commander}}
\def\commanderone{\texttt{Commander1}}
\def\commandertwo{\texttt{Commander2}}
\def\commanderthree{\texttt{Commander3}}
\def\ruler{\texttt{Ruler}}
\def\comrul{\texttt{Commander-Ruler}}
\def\CR{\texttt{C-R}}
\def\nilc{\texttt{NILC}}
\def\gnilc{\texttt{GNILC}}
\def\sevem{\texttt{SEVEM}}
\def\smica{\texttt{SMICA}}
\def\CamSpec{\texttt{CamSpec}}
\def\Plik{\texttt{Plik}}
\def\XFaster{\texttt{XFaster}}
\def\sroll2{\texttt{SRoll2}}

\renewcommand{\d}[0]{\vec{d}}
\renewcommand{\t}[0]{\vec{t}}
\newcommand{\A}[0]{\tens{A}}
\newcommand{\B}[0]{\tens{B}}
\renewcommand{\G}[0]{\tens{G}}
\newcommand{\Y}[0]{\tens{Y}}
\newcommand{\n}[0]{\vec{n}}
\newcommand{\red}[0]{\color{red}}
\newcommand{\green}[0]{\color{green}}
\newcommand{\s}[0]{\vec{s}}
\renewcommand{\a}[0]{\vec{a}}
\newcommand{\m}[0]{\vec{m}}
\newcommand{\f}[0]{\vec{f}}
\newcommand{\F}[0]{\tens{F}}
\newcommand{\T}[0]{\tens{T}}
\newcommand{\Cp}[0]{\tens{C}}
\renewcommand{\L}[0]{\tens{L}}
\newcommand{\g}[0]{\vec{g}}
\newcommand{\N}[0]{\tens{N}}
\newcommand{\M}[0]{\tens{M}}
\newcommand{\iN}[0]{\tens{N}^{-1}}
\newcommand{\iM}[0]{\tens{M}^{-1}}
\newcommand{\w}[0]{\vec{w}}
\renewcommand{\S}[0]{\tens{S}}
\renewcommand{\r}[0]{\vec{r}}
\renewcommand{\u}[0]{\vec{u}}
\newcommand{\q}[0]{\vec{q}}
\renewcommand{\v}[0]{\vec{v}}
\renewcommand{\P}[0]{\tens{P}}
\newcommand{\dt}[0]{d_t}
\newcommand{\di}[0]{d_i}
\newcommand{\nt}[0]{n_t}
\newcommand{\st}[0]{s_t}
\newcommand{\mt}[0]{m_t}
\newcommand{\ft}[0]{f_t}
\newcommand{\Te}[0]{T_{\rm e}}
\newcommand{\EM}[0]{\rm EM}
\newcommand{\mathsc}[1]{{\normalfont\textsc{#1}}}
\newcommand{\hi}{\ensuremath{\mathsc {Hi}}}
\newcommand{\bpbold}{\bfseries{\scshape{BeyondPlanck}}}
\newcommand{\BP}{\textsc{BeyondPlanck}}
\newcommand{\bp}{\textsc{BeyondPlanck}}
\newcommand{\cosmoglobe}{\textsc{Cosmoglobe}}
\newcommand{\Cosmoglobe}{\textsc{Cosmoglobe}}
\newcommand{\lfi}[0]{LFI}
\newcommand{\hfi}[0]{HFI}
\newcommand{\npipe}[0]{\texttt{NPIPE}}
\newcommand{\K}[0]{\textit K}
\newcommand{\Ka}[0]{\textit{Ka}}
\newcommand{\Q}[0]{\textit Q}
\newcommand{\V}[0]{\textit V}
\newcommand{\W}[0]{\textit W}
\newcommand{\e}{\mathrm e}
\newcommand{\cvar}{\ensuremath{c(\vartheta, \varphi, \psi)}}


\def\bC{\tens{C}}
\def\ba{\vec{a}}
\def\ncha{N_\mathrm{cha}}
\def\nfg{N_\mathrm{fg}}

\newcommand{\data}{\vec d}
\newcommand{\ncorr}{\vec n_\mathrm{corr}}
\newcommand{\Dbp}{\Delta_\mathrm{bp}}

%\modulolinenumbers[5]
%\linenumbers

\newcommand{\includegraphicsdpi}[3]{
    \pdfimageresolution=#1  % Change the dpi of images
    \includegraphics[#2]{#3}
    \pdfimageresolution=72  % Change it back to the default
}

\renewcommand{\topfraction}{1.0}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{1.0}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \setcounter{dbltopnumber}{2}    % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.04}	% allow minimal text w. figs
    %   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.9}	% require fuller float pages
	% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.9}	% require fuller float pages

\def\adj{^{\dagger}}
\def\tp{^{\rm T}}
\def\inv{^{-1}}
\def\lm{{\ell m}}

\begin{document}

\title{\bfseries{\Cosmoglobe: Towards likelihood-free CMB cosmological\\ parameter estimation with end-to-end error propagation}}
\input{authors.tex}
%\authorrunning{From BeyondPlanck to Cosmoglobe}
\authorrunning{Eskilt et al.}
\titlerunning{CMB parameter estimation}

\abstract{
  We implement support for likelihood-free cosmological parameter estimation algorithm, as originally proposed by Racine et al.\ (2016), in the world's first end-to-end Bayesian CMB Gibbs sampler called \commander, and quantify its computational efficiency and cost. For a semi-realistic simulation with noise levels similar to \Planck\ LFI 70\,GHz, a maximum multipole of $\ell_{\mathrm{max}}=1300$, and an accepted sky fraction of $f_{\mathrm{sky}}=0.85$, we find that more than 5000 Conjugate Gradient iterations are required to achieve sufficient CMB map accuracy, and that the typical Markov chain correlation length is $\sim$\,100 samples. The net effective cost per independent sample is $\sim$\,5000\,CPU-hrs, which --- when integrated into a full end-to-end Bayesian pipeline --- is to be compared with the sum of all lower-level processing costs. As a relevant example, processing all \Planck\ LFI and \WMAP\ channels within the \cosmoglobe\ Data Release 1 pipeline costs 812\,CPU-hrs, and the addition of the new parameter estimation step discussed here will therefore increase the overall analysis cost of that particular data set by a factor of six. Thus, although technically possible to run already in its current state, future work should aim to reduce the effective cost per independent sample by at least one order of magnitude to avoid excessive runtimes; we argue that this is possible through improved multi-grid preconditioners and/or derivative-based Markov chain sampling schemes. At the same time, we also note that both the Conjugate Gradient mapmaking cost and the Markov chain correlation length depend strongly on the signal-to-noise ratio of the data in question, and next-generation CMB $B$-mode satellite polarization measurements generally have a much lower signal-to-noise ratio than \Planck\ and \WMAP's temperature constraints. On the one hand, this work demonstrates the computational feasiblity of true Bayesian cosmological parameter estimation with end-to-end error propagation for high-precision CMB experiments without likelihood approximations; on the other hand, it also highlights the need for additional optimizations before it is justified for full production-level analysis. 
}

\keywords{ISM: general -- Cosmology: observations, polarization,
    cosmic microwave background, diffuse radiation -- Galaxy:
    general}

\maketitle

%\hypersetup{linkcolor=black}
%\tableofcontents
%\hypersetup{linkcolor=red} 




\section{Introduction}
\label{sec:introduction}

High-precision cosmic microwave background (CMB) measurements provide today the strongest constraints on a wide range of key cosmological parameters \citep[e.g.,][]{planck2016-l06}. Traditionally, these constraints are derived by first compressing the information contained in the high-dimensional raw time-ordered data into pixelized sky maps and angular power spectra \citep[e.g.,][]{bennett2012,planck2016-l02,planck2016-l03}, and this compressed dataset is typically summarized in terms of a low-dimensional power spectrum-based likelihood from which cosmological parameters are derived through Markov Chain Monte Carlo (MCMC) sampling \citep{cosmomc,planck2016-l05}.

A key element in this pipeline procedure is the so-called likelihood function, $\mathcal{L}(C_{\ell})$, where $C_{\ell}$ denotes the angular CMB power spectrum. This function summarizes both the best-fit values of all power spectrum coefficients and their covariances and correlations. The accuracy of the resulting cosmological parameters, both in terms of best-fit values and their uncertainties, correspond directly to the accuracy of this likelihood function. As such, this function must account for both low-level instrumental effects (such as correlated noise and calibration uncertainties) and high-level data selection and component separation effects. In general, this function is neither factorizable nor Gaussian, and except for a few well-known special cases it has no analytical exact and closed form. Unsurprisingly, significant efforts have therefore been spent on establishing computationally efficient and accurate approximations. Perhaps the most well-known example of such is a standard multi-variate Gaussian distribution in units of $C_{\ell}$, with a covariance matrix tuned by end-to-end simulations. A second widely used case is that of a low-resolution multi-variate Gaussian defined in pixel space with a dense pixel-pixel noise covariance matrix. Yet other examples include the log-normal or Gaussian-plus-lognormal distributions, as well as various hybrid combinations of all of these. 

However, as the signal-to-noise ratios of modern CMB data sets continue to increase, the relative importance of systematic uncertainties grows, and these are often highly non-trivial to describe through low-dimensional and simplified likelihood approximations, both in terms of computational expense and accuracy. These difficulties were anticipated and understood more than two decades ago, and an alternative and ground-breaking end-to-end Bayesian approach was proposed independently by \citet{jewell2004} and \citet{wandelt2004}. The key aspect in this framework is global analysis, in which all aspects of the data are modelled and fitted simultaneously, as opposed to compartmentalized as is typically done in traditional CMB pipelines. Technically speaking, this is implemented in the form of an MCMC sampler in a process that is statistically analogous to the very last cosmological parameter estimation step in a traditional pipeline, such as CosmoMC \citep{cosmomc} or Cobaya \citep{Torrado:2020dgo}, with the one fundamental diffence that all parameters in the entire analysis pipeline are sampled over jointly within the MCMC sampler. Thus, while a standard cosmological parameter code typically handles a few tens, or perhaps a hundred, free parameters, the global approach handles billions of free parametes. In practice, the only practical method of dealing with such a large number of parameters of different types proposed to date is through Gibbs sampling, which draws samples from a joint distribution by iterating over all relevant conditional distributions.

At a strictly principled level, a global approach is obviously preferable, simply because better results are in general obtained by fitting correlated parameters jointly rather than separately. However, and equally clearly, this approach is also organizationally and technically more complicated to implement in practice, simply because all aspects of the analysis has to be accounted for simultaneously within one framework. The most advanced effort with this aim published to date is \commanderthree\ \citet{bp05}, which is the world's first end-to-end CMB Gibbs sampler developed by the \BP\ collaboration to re-analyze the \Planck\ Low Frequency Instrument (LFI) observations. This work has subsequently been superceded by the \cosmoglobe\ project\footnote{\url{https://cosmoglobe.uio.no}}, which is a community-wide Open Science collaboration that ultimately aims to perform the same type of analysis for all available state-of-the-art large-scale radio, microwave, and sub-millimeter experiments, and use this to derive one global model of the astrophysical sky. The first \cosmoglobe\ Data Release (DR1) took place in March 2023 \citep{watts2023_dr1}, and included the first joint end-to-end analysis of both \Planck\ LFI and the Wilkinson Microwave Anisotropy Probe (WMAP; \citealp{bennett2012}), resulting in lower systematic residuals in both experiments.

While \commanderthree\ arguably represents the most advanced integrated CMB analysis pipeline in the literature, one key component is still missing before one can claim to perform true end-to-end CMB analysis, and that is in fact cosmological parameter estimation; right now, the highest-level outputs from \commanderthree\ is a set of angular power spectrum samples \citep{bp11}, from which cosmological parameters may be derived using traditional methods \citep{bp12}, much like in a traditional compartmentalized pipeline. The main goal of the current paper is to implement that missing step, to quantify its overall computational cost, and to identify potential bottlenecks.

The numerical challenges involved in implementing likelihood-free cosmological parameter estimation in Gibbs sampling was originally explored and partially resolved by \citet{jewell:2009}. In short, the fundamental difficulty lies in the fact that a strict Gibbs sampler only allows parameter moves in directions that are orthogonal, and this leads to a long Markov chain correlation length in the low signal-to-noise regime, where the CMB sky signal and the theoretical model are nearly perfectly degenerate. To circumvent this problem, \citet{racine:2016} therefore proposed a joint sampling step for both the sky signal and theory model that allow these to move relatively quickly throughout the full parameter space. However, they only demonstrated the method with a proof-of-concept and stand-alone Python implementation using an ideal case. In the current paper, we implement the same method in \commanderthree, and apply it to simulations with realistic noise and sky coverages. From a formal point-of-view, this work therefore completes the end-to-end aspect of the \commanderthree\ code. 

The rest of the paper is organized as follows. In Sect.~\ref{sec:methods} we provide a brief review of both the \cosmoglobe\ Gibbs sampler method in general and the specific cosmological parameter sampler described by \citet{racine:2016}. In Sect.~\ref{sec:results}, we validate our implementation by comparing it to two special cases, namely Cobaya coupled to a uniform and fullsky case and a specical-purpose Python Gibbs sampler for a case with uniform noise but a constant latitude Galactic mask. In the same section, we also characterize the computational costs of the new sampling step for various data configurations. Finally, we summarize and conclude in Sect.~\ref{sec:conclusions}.

\section{Algorithms}
\label{sec:methods}

We start by briefly reviewing the Bayesian framework for global end-to-end analysis as described by \citet{bp01,watts2023_dr1} and the cosmological parameter estimation algorithm proposed by \citet{racine:2016}, and discuss how these may be combined in \commanderthree\ \citep{bp03}.

\subsection{Bayesian end-to-end CMB analysis}

The main algorithmic goal of the \cosmoglobe\ framework is to derive a numerical representation of the full posterior distribution $P(\omega|\d)$, where $\omega$ is the set of all free parameters and $\d$ represents all available data. In this notation, $\omega$ simultaneously accounts for instrumental, astrophysical and cosmological parameters, and are typically explicitly defined by writing down a signal model, for instance taking the form
\begin{equation}
    \label{eq:data_model}
    \d = \A \a + \n,
\end{equation}
where $\d$ is the data vector; $A$ represents some set of linear operations; $\a$ denotes some set of amplitude parameters; and $\n$ is instrumental noise. In practice, one typically assumes that the noise is Gaussian distributed with a covariance matrix $\N$, and the likelihood may therefore be written as
\begin{equation}
  \mathcal{L}(\omega) = P(\d|\omega) \propto e^{-\frac{1}{2}\left(\d-\s(\omega)\right)^t\N^{-1}\left(\d-\s(\omega)\right)}.
\end{equation}
The full posterior distribution reads $P(\omega|\d) \propto \mathcal{L}(\omega)P(\omega)$, where $P(\omega)$ represents some set of user-defined priors.

As a concrete example, \cosmoglobe\ Data Release 1 \citep{watts2023_dr1} adopted the following data model to describe \Planck\ LFI and \WMAP,
\begin{equation}
	\label{eq:model}
	\d =\G\P\B\M\boldsymbol \a+ \s^\mathrm{orb}
	+\s^\mathrm{fsl} + \s^\mathrm{inst}+ \n^\mathrm{corr}+\n^\mathrm w,
\end{equation}
where $\mathsf G$ is a time-dependent gain factor; $\mathsf P$ is a pointing matrix;
$\B$ denotes instrumental beam convolution; $\mathsf M$ is a mixing matrix that describes the amplitude of a given sky component at a given frequency; $\a$ describes the amplitude of each component at each point in the sky; $\s^\mathrm{orb}$ is the orbital CMB dipole; $\s^\mathrm{fsl}$ is a far sidelobe contribution; $\s^\mathrm{inst}$ is an instrument-specific correction term; and $\n^\mathrm{corr}$ denotes correlated noise. This expression directly connects important instrumental effects (e.g., gains, beams and correlated noise) with the astrophysical sky (e.g., $\M$ and $\a$), and provides a well-defined model for the data at the lowest level; this model is what enables global modelling.

For our purposes, the most important sky signal component is the CMB anisotropy field, $\s_{\mathrm{CMB}}$. The covariance of this component reads $\S = \langle \s_{\mathrm{CMB}}^T \s_{\mathrm{CMB}}\rangle$. Under the assumption of a statistically isotropic universe, this matrix is diagonal in harmonic space, $\langle s_{\ell m} s^*_{\ell' m'}\rangle = C_{\ell m}(\theta) \delta_{\ell\ell'}\delta_{mm'}$, where $s(\hat{n}) = \sum_{\ell,m} s_{\ell m} Y_{\ell m}(\hat{n})$ is the usual spherical harmonics expansion and $C_{\ell}$ is called the angular power spectrum. This power spectrum depends directly on a small set of cosmological parameters, $\theta$, and may for most universe models be computed efficiently by Boltzmann solvers such as CAMB or CLASS.

With this notation, the goal is now to compute $P(\omega|\d)$, where $\omega = \{\G, \n_{\mathrm{corr}}, \M, \a, \theta, \ldots\}$. Unfortunately, directly evaluating or sampling from this distribution is unfeasible. In practice, we therefore resort to Markov Chain Monte Carlo sampling in general, and Gibbs sampling in particular, which is a well-established method for sampling from complicated multivariate distribution by iterating over all conditional distributions. For the data model described above, this process may be described schematically in the form of a Gibbs chain, 
  \begin{alignat}{9}
    \label{eq:gain_samp_dist}\G &\,\leftarrow          P(\G&\,               \mid \data, &\,\phantom{\G,} &\,\ncorr,&\,\M, &\,\a, &\,\theta)\\
    \label{eq:ncorr_samp_dist} \ncorr &\,\leftarrow    P(\ncorr&\,        \mid \data, &\,\G, &\,\phantom{\ncorr,}  &\,\M, &\,\a, &\,\theta)\\
    \label{eq:beta_samp}\M &\,\leftarrow                     P(\M &\, \mid \data, &\,\G, &\,\ncorr, &\,\phantom{\M}, &\,\a, &\,\theta)\\
    \a &\,\leftarrow                                   P(\a&\,            \mid \data, &\,\G, &\,\ncorr, &\,\M, &\,\phantom{\a,} &\,\theta)\\
    \theta &\,\leftarrow                             P(\theta &\,         \mid \data, &\,\G, &\,\ncorr, &\,\M, &\,\a,&\,\phantom{\theta})\label{eq:param_samp},\\\vspace*{-4mm}
     &&\vdots                             & & & & & & \nonumber
    \end{alignat}
  where $\leftarrow$ indicates replacing the parameter on the left-hand side with a random sample from the distribution on the right-hand side.

As described by \citet{bp01,watts2023_dr1}, this process has now been implemented in \commanderthree\ by the \BP\ and \Cosmoglobe collaborations, and demonstrated to work very well in practice with \Planck\ LFI and \WMAP. However, in the last step of the existing code, the last step only supports power spectrum estimation, i.e., $\theta = C_{\ell}$; the goal of the current paper is to replace that step with actual cosmological parameter estimation, in which $\theta$ takes on the usual form of the dark matter density $\Omega_\mathrm{c}$, the Hubble constant $H_0$, the spectral index of scalar perturbation $n_\mathrm{s}$ etc.

\subsection{Joint sampling of CMB sky signal and cosmological parameters}

The challenge of sampling cosmological parameters efficiently within an end-to-end Gibbs sampling framework has been addressed in the literature for almost two decades, starting with \citet{jewell2004} and \citet{wandelt2004}. As pointed out by \citet{eriksen:2004}, a major difficulty regarding this method is a very long correlation length in the low signal-to-noise regime, i.e., at high multipoles. Intuitively, the origin of this problem lies in the fundamental Gibbs sampling algorithm itself, namely that it only allows for parameter variations parallel or orthogonal to the coordinate axes of each parameter in question, and not diagonal moves. For highly degenerate distributions, this makes it very expensive to move from tail of the distribution to the other. Another way to visualize this is simply by noting that the Markov chain step size in a pure Gibbs sampling is given by CMB cosmic variance alone, while the full posterior distribution width is defined by both signal and noise; the Gibbs sampler therefore moves quickly in the high signal-to-noise regime, but slowly in the low signal-to-noise regime. A solution to this problem was proposed by \citet{jewell:2009}, who introduced a joint CMB sky signal and spectral parameter move. This idea was further refined by \citet{racine:2016}, who noted that faster convergence could be obtained by only rescaling the low signal-to-noise multipoles. In this section, we give a brief summary of these ideas, and refer the interested reader to the original papers for full details.

\subsubsection{Exact likelihood}
\label{sec:exact-likelihood}

In principle, we could evaluate the exact marginal likelihood of the cosmological parameters given the data, 
\begin{equation}
    P(\boldsymbol{\theta} | \boldsymbol{d}) \propto \frac{e^{-\frac12 \boldsymbol{d}^T (\boldsymbol{A}^T \boldsymbol{S}(\boldsymbol{\theta}) \boldsymbol{A} + \boldsymbol{N})^{-1}\boldsymbol{d}}}{\sqrt{\text{det}\left(\boldsymbol{A}^T \boldsymbol{S}(\boldsymbol{\theta}) \boldsymbol{A} + \boldsymbol{N}\right)}}.
\end{equation}
This, however, becomes intractible for modern CMB experiments as the computational expense grows as $\mathcal{O}(N_p^3)$, where $N_p$ is the number of pixels.

In the unrealistic case of uniform noise and no applied mask, this is likelihood can be simplified and becomes computionally tractable. Using the notation that the observed beam-convolved map with noise of the CMB is $\hat{C}^{\mathrm{o}}_{\ell} = \frac{1}{2\ell+1}\sum_m d_{\ell m}d^*_{\ell m}$, and assuming isotropic white noise $N_{\ell m \ell'm'} = N_\ell \delta_{\ell \ell'}\delta_{mm'}$, it is straightforward to show that this likelihood simplifies to
\begin{align}
    \nonumber
    \log P(\boldsymbol{\theta} | \boldsymbol{d}) = \sum_{\ell} -\frac{2\ell+1}{2} &\bigg[\frac{\hat{C}^{\mathrm{o}}_{\ell}}{A_\ell^2 C_{\ell}(\boldsymbol{\theta}) + N_\ell}\\
    -\ln &\left(\frac{\hat{C}^{\mathrm{o}}_{\ell}}{A_\ell^2 C_{\ell}(\boldsymbol{\theta}) + N_\ell} \right) \bigg],
\end{align}
where we have removed constant terms. Including polarization as well, this generalizes to \citep{Hamimeche:2008ai}
\begin{align}
    \nonumber
    \log P(\theta | \boldsymbol{d}) = \sum_\ell -\frac{2\ell+1}{2} \bigg[&\mathrm{Tr}\left(\hat{\boldsymbol C}^{\mathrm{o}}_{\ell}  \left(\boldsymbol{A}^T \boldsymbol{C}_{\ell} \boldsymbol{A} + \boldsymbol{N}\right)^{-1}\right)\\
    - &\ln \det \left(\hat{\boldsymbol C}^\mathrm{o}_{\ell}  \left(\boldsymbol{A}^T \boldsymbol{C}_{\ell} \boldsymbol{A} + \boldsymbol{N}\right)^{-1}\right)\bigg].
\end{align}
Here, we have removed the explicit dependence of $\theta$ in the theoretical $\Lambda$CDM power spectra, $\boldsymbol{C}_{\ell} = \boldsymbol{C}_{\ell}(\boldsymbol{\theta})$. We also use
\begin{align}
    \hat{\boldsymbol C}^{\mathrm{o}}_{\ell} &= \frac{1}{2\ell+1}\sum_m \boldsymbol{d}_{\ell m}^T \boldsymbol{d}^*_{\ell m},\\
    \boldsymbol{C}^{\Lambda\mathrm{CDM}}_{\ell} &= \begin{bmatrix}
        C^{TT}_\ell & C^{TE}_\ell & C^{TB}_\ell\\
        C^{TE}_\ell & C^{EE}_\ell & C^{EB}_\ell\\
        C^{TB}_\ell & C^{EB}_\ell & C^{BB}_\ell,
    \end{bmatrix}
\end{align}
where we define the vectors of spherical harmonics coefficients, ${\boldsymbol{d}_{\ell m}^T = [d^T_{\ell m}, d^E_{\ell m}, d^B_{\ell m}]}$. We will use Cobaya \citep{Torrado:2020dgo} to evaluate this likelihood in the case of uniform noise on fullsky data to verify our \commander3\ implementation.

\subsubsection{Gibbs sampling}
\label{sec:gibbs}

However, a more realistic set-up includes non-uniform noise and a Galactic mask, and the exact likelihood becomes computationally too expensive. Using Bayes theorem, the joint posterior density $P(\boldsymbol{\theta}, \boldsymbol{s} | \boldsymbol{d})$ can be written as
\begin{align}
    \nonumber
    P(\boldsymbol{\theta}, \boldsymbol{s} | \boldsymbol{d}) &= \frac{P(\boldsymbol{\theta}, \boldsymbol{s}, \boldsymbol{d})}{P(\boldsymbol{d})} = P(\boldsymbol{d} | \boldsymbol{s})P(\boldsymbol{s}| \boldsymbol{\theta})\frac{P(\boldsymbol{\theta})}{P(\boldsymbol{d})}\\
    \label{eq:joint-posterior}
    &= \frac{e^{-\frac12 \left(\boldsymbol{d}-\boldsymbol{A}\boldsymbol{s} \right)^T \boldsymbol{N}^{-1}\left(\boldsymbol{d}-\boldsymbol{A}\boldsymbol{s} \right)}}{\sqrt{\mathrm{det}\left(\boldsymbol{N}\right)}}
    \frac{e^{-\frac12 \boldsymbol{s}^T \boldsymbol{S}^{-1}\boldsymbol{s}}}{\sqrt{\mathrm{det}\left(\boldsymbol{S}\right)}}\frac{P(\boldsymbol{\theta})}{P(\boldsymbol{d})}
\end{align}
From this joint posterior distribution, we can retrieve $P(\boldsymbol{\theta} | \boldsymbol{d})$ by marginalizing over $\boldsymbol{s}$.

In the spirit of the Gibbs sampling framework of \cosmoglobe, one might think that the best approach is to Gibbs sample both the sky signal $\boldsymbol{s}$ and cosmological parameters $\boldsymbol{\theta}$
\begin{align}
    \boldsymbol{s}^{i+1} &\leftarrow P(\boldsymbol{s} | \boldsymbol{\theta}^{i}, \boldsymbol{d}),\\
    \label{eq:theta-gibbs}
    \boldsymbol{\theta}^{i+1} &\leftarrow P(\boldsymbol{\theta} | \boldsymbol{s}^{i+1}, \boldsymbol{d}).
\end{align}
Gibbs sampling requires that we know the posterior distribution of each step. The first of these distributions is discussed in \cite{jewell2004} and \cite{wandelt2004}. As we condition on $\boldsymbol{\theta}$ and $\boldsymbol{d}$, the variance of the sky signal comes from both cosmic variance and instrumental noise, and one can show that the distribution becomes
\begin{equation}
    P(\boldsymbol{s} | \boldsymbol{\theta}, \boldsymbol{d}) \propto e^{-\frac12 \left(\boldsymbol{s} - \boldsymbol{\hat{s}}\right)^T \left(\boldsymbol{S}^{-1} + \boldsymbol{A}^T\boldsymbol{N}^{-1}\boldsymbol{A}\right) \left(\boldsymbol{s} - \boldsymbol{\hat{s}}\right)},
\end{equation}
where we have defined the deterministic mean field map
\begin{equation}
\label{eq:mean-field-map}
\boldsymbol{\hat{s}} \equiv \left[\boldsymbol{S}^{-1} + \boldsymbol{A}^t \boldsymbol{N}^{-1}\boldsymbol{A} \right]^{-1} \boldsymbol{A} \boldsymbol{N}^{-1} \boldsymbol{d}.
\end{equation}
This is a multivariate Gaussian distribution with mean $\boldsymbol{\hat{s}}$ and a covariance matrix $\left[\boldsymbol{S}^{-1} + \boldsymbol{A}^t \boldsymbol{N}^{-1}\boldsymbol{A} \right]^{-1}$ which can be sampled from solving the constrained realization equation:
\begin{equation}
    \label{eq:mapmakingeq}
    \left[\boldsymbol{S}^{-1} + \boldsymbol{A}^t \boldsymbol{N}^{-1}\boldsymbol{A} \right]\boldsymbol{s} = \boldsymbol{A} \boldsymbol{N}^{-1} \boldsymbol{d} + \boldsymbol{S}^{-\frac{1}{2}}\boldsymbol{w}_0 +\boldsymbol{A N}^{-\frac{1}{2}}\boldsymbol{w}_1,
\end{equation}
where $\boldsymbol{w}_0$ and $\boldsymbol{w}_1$ are randomly drawn Gaussian maps with unit variance and zero mean.

We decompose the sky map into the mean field map given in Eq.~\eqref{eq:mean-field-map} and the fluctuation map $\boldsymbol{\hat{f}}$,
\begin{equation}
\label{eq:fluc-map}
\boldsymbol{\hat{f}} \equiv \left[\boldsymbol{S}^{-1} + \boldsymbol{A}^t \boldsymbol{N}^{-1}\boldsymbol{A} \right]^{-1} \left(\boldsymbol{S}^{-\frac{1}{2}}\boldsymbol{w}_0 +\boldsymbol{A N}^{-\frac{1}{2}}\boldsymbol{w}_1 \right)
\end{equation}
so that $\boldsymbol{s} = \boldsymbol{\hat{s}} + \boldsymbol{\hat{f}}$.
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/s_hat_f_hat.pdf}
	\caption{\label{fig:sky_map}A constrained realization of a simulated CMB with a Galactic and point-source mask is decomposed into the mean field map $\hat{s}_{\ell m}$ and the fluctuation map $\hat{f}_{\ell m}$. The bottom panels shows the full sky map of the sample, $s_{\ell m} = \hat{s}_{\ell m} + \hat{f}_{\ell m}$.}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/sigma_ell.pdf}
	\caption{\label{fig:sigma_ell}The power spectra for the sky maps in Fig.~\ref{fig:sky_map}. This is compared to the $\Lambda$CDM power spectra used to generate the data.}
\end{figure}

In a setup with uniform noise on fullsky temperature data, this can be solved exactly in spherical harmonics space:
\begin{align}
    \label{eq:hat_s_approx}
    \hat{s}_{\ell m} &= d_{\ell m}\frac{A_{\ell}C_{\ell}}{N_\ell + A_{\ell}^2C_{\ell}},\\
    \label{eq:hat_f_approx}
    \hat{f}_{\ell m} &= w_{0\ell m}\frac{N_{\ell}\sqrt{C_{\ell}}}{N_\ell + A_{\ell}^2C_{\ell}}+w_{1\ell m}\frac{\sqrt{N_{\ell}}A_{\ell}C_\ell}{N_\ell + A_{\ell}^2C_{\ell}}.
\end{align}
In a more realistic scenario with non-uniform noise and masking, Eq.~\eqref{eq:mapmakingeq} becomes a computationally hard equation to solve. In \commanderthree, it is solved by the method of Conjugant Gradient which is the most time consuming part of the sampling process. In Appendix A, we show how this equation can be simplified for a constant latitude mask with isotropic noise. We made a Python script that does this calculation to verify our algorithm implementation in \commanderthree.

We visualize a constrained realization generated in \commanderthree\ in Fig.~\ref{fig:sky_map}. Here, we use a realistic Galactic plane and point-source mask and non-uniform noise. This configuration corresponds to the realistic setup we will discuss later in Sec.~\ref{sec:results}. Equations \eqref{eq:hat_s_approx} and \eqref{eq:hat_f_approx} are not valid due to $\boldsymbol{N}$ being non-diagonal in spherical harmonics space, and we need to solve the dense matrix equations in Eqs.~\ref{eq:mean-field-map} and \ref{eq:fluc-map}. The top panel shows the mean field map $\hat{s}_{\ell m}$. As the mask hides everything in the Galactic plane, we only have knowledge of large scale fluctuations inside the masked regions from the information outside of the mask. However, we lack knowledge of smaller scales in the masked regions, yielding a strong signal in the stocastic fluctuation map $\hat{f}_{\ell m}$. At even smaller scales outside of the masked regions, noise dominates and the uncertainty from noise is shown at very high multipoles in $\hat{f}_{\ell m}$. Putting both maps together gives us the sky map of this sample, $s_{\ell m} = \hat{s}_{\ell m} + \hat{f}_{\ell m}$.

We show the temperature and polarization power spectra of this sky map in Fig.~\ref{fig:sigma_ell}. At lower multipoles there is a high signal-to-noise (except for $BB$) and the mean field map $\hat{s}_{\ell m}$ dominates. But as we go to higher multipoles, noise starts dominating and all power comes from the fluctuation field $\hat{f}_{\ell m}$.

The second step of the Gibbs sampling in Eq.~\eqref{eq:theta-gibbs} can be sampled by an inverse Wishart ditribution, but we will not be using this step further in this work.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/parameter-estimation.jpg}
	\caption{\label{fig:illustration}Illustration of consequetive samples of the Gibbs sampler and the method of \cite{racine:2016}. The Gibbs sampler perform poorly in the low signal-to-noise regime as it requires a much larger number of samples to explore the posterior distribution as compared to the high signal-to-noise region. The joint sampler in the lower panels performs well in both regimes as it allows the next sample to move diagonally in parameter space as we sample $\boldsymbol{\theta}$, yielding $C_\ell(\boldsymbol{\theta})$ in the figure.}
\end{figure}

The Gibbs sampling method works well in high signal-to-noise regions, but performs poorly in the opposite limit. This can be understood by Eq.~\eqref{eq:fluc-map} when we have high instrumental noise, $\boldsymbol{N} \rightarrow \infty$, then we have no information of $s_{\ell m}$ from the data $d_{\ell m}$, and the sky map becomes only a realization of the variance $C_{\ell}$, i.e. $s_{\ell m} \rightarrow \hat{f}_{\ell m} = \sqrt{C_{\ell}} w_{0\ell m}$. Since we have no information of the mean field map $\hat{s}_{\ell m} \rightarrow 0$, we also do not know $C_\ell$. Hence, we get a strong degeneracy between the $C_\ell$ and $s_{\ell m}$. Gibbs sampling is well-known of performing poorly for strongly correlated parameters as you can not move diagonally in parameter space from one end of the joint posterior distribution to the other. This problem is shown in the top panel of Fig.~\ref{fig:illustration}, which shows the difference in the correlation length of Gibbs sampling between high and low signal-to-noise limits.

\subsubsection{Joint sampling}
\label{sec:joint-sampling}
A solution to the poor correlation length of the Gibbs sampling in low signal-to-noise regions was proposed by \cite{jewell:2009} further developed in \cite{racine:2016}. The authors suggested a Metropolis-Hasting sampler with a general proposal rule for $\theta$ and a rescaling of the fluctutation term of the sky signal. This allows the sampler to move diagonally in parameter space and more efficiently explore the full joint posterior distribution, as depicted in Fig.~\ref{fig:illustration}.

More precisely, one would perform
\begin{align}
    \boldsymbol{\theta}^{i+1} 
 &\leftarrow w(\boldsymbol{\theta} |\boldsymbol{\theta}^i)\\
 \boldsymbol{s}^{i+1} &\leftarrow P(\boldsymbol{s} | \boldsymbol{\theta}^{i+1}, \boldsymbol{d}),
\end{align}
for each Gibbs iteration, where the arrow denotes drawing a sample from the distribution on the right-hand side. $w(\theta |\theta^i)$ is the proposal distribution of the cosmological parameters. We use a multivariate Gaussian function,
\begin{equation}
w(\boldsymbol{\theta} |\boldsymbol{\theta}^i) = e^{-\frac12 \left(\boldsymbol{\theta} - \boldsymbol{\theta}^i \right)^T \boldsymbol{C}_{\boldsymbol{\theta}}^{-1}\left(\boldsymbol{\theta} - \boldsymbol{\theta}^i \right)}.
\end{equation}
For optimal chain convergence, the proposal variance $\boldsymbol{C}_{\boldsymbol{\theta}}$ should resemble the posterior distribution. The proposal variance we use is created from the samples of an earlier chain created with a diagonal proposal matrix.

They key altercation is to rescale the fluctuation map of the last accepted sample,
\begin{equation}
    \hat{f}_{\ell m}^{\textrm{scaled},\, i+1} = \left(\boldsymbol{C}^{i+1}_{\ell}\right)^{1/2}\left(\boldsymbol{C}^{i}_{\ell}\right)^{-1/2} \hat{f}_{\ell m}^{i},
\end{equation}
so that the map of sample $i+1$ is
\begin{equation}
    s_{\ell m}^{i+1} = \hat{s}_{\ell m}^{i+1} + \left(\boldsymbol{C}^{i+1}_{\ell}\right)^{1/2}\left(\boldsymbol{C}^{i}_{\ell}\right)^{-1/2} \hat{f}_{\ell m}^{i}.
\end{equation}
In low signal-to-noise regions, this rescaling allows diagonal moves in $s_{\ell m} - C_\ell$ space of Fig.~\ref{fig:illustration}. The acceptance rate for this proposal was calculated in \cite{racine:2016} to be
\begin{equation}
    \label{eq:acceptance-rate}
    A = \mathrm{min}\left[1, \frac{\pi(\boldsymbol{\theta}^{i+1})}{\pi(\boldsymbol{\theta}^i)} \frac{w(\boldsymbol{\theta}^{i} |\boldsymbol{\theta}^{i+1})}{w(\boldsymbol{\theta}^{i+1} |\boldsymbol{\theta}^i) }\frac{P(\boldsymbol{\theta}^{i+1})}{P(\boldsymbol{\theta}^i)} \right].
\end{equation}
Here, $P(\boldsymbol{\theta})$ is the prior on $\boldsymbol{\theta}$ and
\begin{align}
    \nonumber
    \pi(\boldsymbol{\theta}^{i}) = \mathrm{exp}\bigg[&-\frac12 \left(\boldsymbol{d}-\boldsymbol{A}\boldsymbol{\hat{s}}^i\right)^T \boldsymbol{N}^{-1}\left(\boldsymbol{d}-\boldsymbol{A}\boldsymbol{\hat{s}}^i\right)\\
    &-\frac12\boldsymbol{\hat{s}}^{i,T} \boldsymbol{S}^{i, -1}\boldsymbol{\hat{s}}^i -\frac12 \boldsymbol{\hat{f}}^{i, T}\boldsymbol{A}^T\boldsymbol{N}^{-1} \boldsymbol{A}\boldsymbol{\hat{f}}^i\bigg],
\end{align}
where we have omitted the $\theta^i$ dependence of $\hat{s}^i$, $\hat{f}^i$ and $S^i$. Our proposal matrix for the cosmological parameters is symmetric, $w(\boldsymbol{\theta}^{i} |\boldsymbol{\theta}^{i+1}) = w(\boldsymbol{\theta}^{i+1} |\boldsymbol{\theta}^{i})$, and hence we can remove them from Eq.~\eqref{eq:acceptance-rate}.

We note that the acceptance rate will use the scaled fluctuation term, $f_{\ell m}^{\textrm{scaled}, i+1}$, for sample $i+1$ instead of the calculated fluctuation term in Eq.~\eqref{eq:mapmakingeq}. Hence, we only need to calculate $f_{\ell m}^{i+1}$ from this equation if the sample is accepted. This allows us to save roughly half the computational time of discarded samples as compared to accepted samples.

The algorithm can be broken down into these steps:
\begin{enumerate}
    \item Start with an initial value $\boldsymbol{\theta}^0$. From that, calculate the power spectra $\boldsymbol{S}^0$, the mean-field map $\boldsymbol{\hat{s}}^0$ and the fluctuation map $\boldsymbol{\hat{f}}^0$ from the cosmological parameters.
    \item Pick a new cosmological parameter sample, $\boldsymbol{\theta}^{i+1}$. Calculate $\boldsymbol{S}^{i+1}$, $\boldsymbol{\hat{s}}^{i+1}$ and the \textit{scaled} fluctuation term $f_{\ell m}^{\textrm{scaled}, i+1}$.
    \item Calculate the acceptance term of Eq.~\ref{eq:acceptance-rate}, and accept/reject according to the regular Metropolis-Hasting rule.
    \item If the sample is accepted, then calculate $f_{\ell m}^{i+1}$. This term will in the next iteration become $f_{\ell m}^{i}$ which will appear in the acceptance probability and the equation for $f_{\ell m}^{\textrm{scaled}, i+1}$. The fluctuation map is not necessary to calculate if the sample is discarded.
    \item Iterate 2-4.
\end{enumerate}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/dist_posterior_no_mask.pdf}
	\caption{\label{fig:nomask}Parameter estimation on a CMB realization with isotropic noise. We use $N_{\mathrm{side}}=512$ and the beam convolve using the beam window functions of 70\,GHz band of Planck. No mask is applied.}
\end{figure}

In this work, we sample the 6 $\Lambda$CDM parameters, i.e. $\boldsymbol{\theta}=(\Omega_{\textrm{b}}h^2, \Omega_{\textrm{CDM}}h^2, H_0, \tau, A_s, n_s)$. Here, $\Omega_\mathrm{b}$ and $\Omega_\mathrm{CDM}$ are the current density of baryonic and cold dark matter, respectively. $H_0$ si the Hubble constant today, while $h$ is the normalized Hubble constant, $h=\frac{H_0}{100\,\mathrm{km s}^{-1} \mathrm{MPc}^{-1}}$. $\tau$ is the optical depth at reionization, $A_s$ is the amplitude and $n_s$ is the tilt of the scalar primordial power spectrum. We use \texttt{CAMB}\footnote{\url{https://github.com/cmbant/CAMB}} to generate the $\Lambda$CDM power spectra $\boldsymbol{C}_{\ell}(\boldsymbol{\theta})$ \citep{Lewis:1999bs}.


\section{Results}
\label{sec:results}

We implement the algorithm into the \commanderthree\ framework with the aim of being a part of the full Gibbs sampling chain alongside sampling of instrumental and astrophysical parameter in future work.

To verify the implementation, we also created a special-prupose Python script that samples cosmological parameters on a CMB simulation with uniform noise and a constant latitude mask. We show how we calculate an analytic expression for $N^{-1}_{\ell m \ell' m'}$ in Appendix A, and how that simplifies matrix equations such as the map making equation and acceptance rate. This Python script has been made publically available\footnote{\url{https://github.com/LilleJohs/}}. We also use Cobaya \citep{Torrado:2020dgo} to verify uniform noise on full sky data.

We create two sets of CMB simulations: One with uniform noise and one with non-uniform noise using a realization of a randomly picked RMS noise map of the 70\,GHz from the official BeyondPlanck chain. Both simulations are created using $N_{\textrm{side}}=512$ and the public Planck Data Release 4 70\,GHz beams \citep{npipe}.

First, we run the uniform noise simulation without a mask in Cobaya, Python and \commanderthree\ to make sure the three implementations agree with each other. This case can be solved exactly using the likelihood presented in Section \ref{sec:exact-likelihood} which we do using Cobaya \citep{Torrado:2020dgo}, while the Python script implements the method of \citet{racine:2016}. Fig.~\ref{fig:nomask} shows the posterior distribution of the cosmological parameters from 50 000 samples each of \commanderthree, Python and Cobaya, and we find excellent agreement between the three implementations.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/dist_posterior_10_mask.pdf}
	\caption{\label{fig:mask10}Cosmological parameter estimation on same simulation as in Fig.~\ref{fig:nomask} but a constant latitude mask is applied so that the sky fraction is 90\%.}
\end{figure}

We then proceed by adding a constant latitude mask of latitude $\arcsin(0.1) \sim 5.7^\circ$ as this gives the sky coverage $f_{\mathrm{sky}} = 0.90$. $\boldsymbol{N}$ is no longer diagonal and the brute-force method of Section \ref{sec:exact-likelihood} will be computationally intractable. 

We use the Python implementation for this case, but we note that it is computationally demanding with a single accepted sample taking 853 CPU-hours. Note, that an accepted sample takes longer to calculate than a discarded sample as we only calculate the fluctuation map $f^i_{\ell m}$ if sample $i$ is accepted.

We show the posterior distribution of $\sim 5000$ Python samples and $\sim 9000$ \commanderthree\ samples in Fig.~\ref{fig:mask10}. Although we have a lack of independent samples due to the long correlation length and computational cost, we find good an agreement between the two implementations.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/auto_correlation.pdf}
	\caption{\label{fig:autocorrelation}Auto correlations of some cosmological parameters.}
\end{figure}

We calculate the autocorrelation function for three cosmological parameters for three chains in Fig.~\ref{fig:autocorrelation}. We show the brute-force likelihood method in Cobaya which has a correlation length $\lesssim 10$. Cosmological parameter sampling using the likelihood-free estimator in \commanderthree\ for the same case is also shown and exhibits a correlation length 5 times higher than the brute-force likelihood.

Although we are sampling the same simulated CMB map without a mask in \commanderthree\ and Cobaya, we expect a higher correlation length as the algorithm of \citet{racine:2016}  the sky signal $s_{\ell m}$. We also show in addition the case with a $10\%$ mask, which increases the correlation length as compared to full-sky.

\subsection{Realistic mask and noise}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/realistic_chain.pdf}
	\caption{\label{fig:traceplot}Traceplot of the cosmological parameters for the $70\,$GHz case. The dashed lines show the input value of the CMB simulation.}
\end{figure}

We then turn to a more realistic case: We make a realization of a 70\,GHz noise map and a realistic Galactic and point source mask of $f_{\mathrm{sky}}=0.85$. Our Python implementation can not sample this case as it neither supports anisotropic noise nor a realistic mask. Hence, we can only rely on the CG iterator of \commanderthree3.

In Fig.~\ref{fig:traceplot} we show the traceplots of the cosmological parameters from this run. We did not include this chain into the autocorrelation Fig.~\ref{fig:autocorrelation} as we have few samples for it converge, but it is clear by eye that the correlation length is $\mathcal{O}(100)$.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/run_time.pdf}
	\caption{\label{fig:runtime}CPU-hours per accepted sample. This is compared to the 812 CPU-hours to generate one single end-to-end sample of \planck\ LFI and \wmap\ channels in red.}
\end{figure}

We are limit by a lack of independent samples due to the large correlation length and a very high cost computational cost of one sample. We show the current CPU-hour per accepted joint sample in Fig.~\ref{fig:runtime} for the different configuration. These cases are compared to the 812 CPU-hours it takes to generate one Cosmoglobe DR1 Gibbs sample from TOD in red. This shows that more works need to be done to lower the computational time to generate a joint sample, as we want to include the methods into the Cosmoglobe framework on actual data.

The computational time is dominated by solving Eq.~\eqref{eq:mapmakingeq} by the method of Conjugant Gradient (CG). For the 70\,GHz case, we currently require 5000 CG iterations to generate one sample of the CMB which takes 46 CPU-hours. With a correlation length of $\sim 100$ samples, we then find 4600 CPU-hours to generate an individual sample of the cosmological parameters. Hence, work is needed to lower the computational cost of solving Eq.~\eqref{eq:mapmakingeq} as we do not see a way to shorten the correlation length of the algortihm.

Lowering this number can be done by better preconditioning and Hamiltonian sampling and blablabla \textcolor{red}{This part is for HK}

\section{Conclusions}
\label{sec:conclusions}

We have shown that the algorithm presented in \cite{racine:2016} works well in the \commanderthree framework for anisotropic noise and mask.

\bibliographystyle{../common/aa}

\bibliography{../common/Planck_bib,../common/CG_bibliography}

\appendix

\section{Analytic expression for a constant latitude mask}
\label{sec:appendixA}


The goal of this appendix is to show how we can calculate $N_{\ell m \ell' m'}^{-1}$ analytically for a constant latitude mask and uniform noise, and how this expression makes the map making equation computationally fast to solve. The noise can be written as this in pixel space:
$$
\left(N^{-1} \right)_{pp'} = \frac{N_{\mathrm{pix}}}{\sigma^2} \delta_{pp'} H(|\theta(p) -\pi/2|-b).
$$
where $H$ is the Heavyside function, meaning that we mask every pixel $p$ where $|\theta(p) -\pi/2| < b$ for some latitude $b$ in radians. For a masked pixel, $\left(N^{-1} \right)_{pp}=0$ which means that $N_{pp} = \infty$ for that pixel, as we want.

Using that
$$
\left(N^{-1}\right)_{pp'} = \sum_{\ell m}\sum_{\ell' m'} \left(N^{-1}\right)_{\ell m \ell'm'} Y_{\ell m}\left(p\right)Y^*_{\ell' m'}\left(p'\right),
$$
where $Y_{\ell m}\left(p\right) = Y_{\ell m}\left(\hat{n}(p)\right)$ and $\hat{n}(p)$ are the spherical coordinates for pixel $p$, we can transform to spherical harmonics space,
\begin{align}
\nonumber
\left(N^{-1}\right)_{\ell m \ell' m'} &= \sum_{p p'}\left(N^{-1}\right)_{pp'}Y^{*}_{\ell m}(p)Y_{\ell' m'}(p')\\
\nonumber
&= \frac{N_{\mathrm{pix}}}{\sigma^2}\sum_{p p'} Y^{*}_{\ell m}(p)Y_{\ell' m'}(p') \delta_{pp'} H(|\theta -\pi/2|-b)\\
\nonumber
&= \frac{N_{\mathrm{pix}}}{\sigma^2}\sum_{p } Y^{*}_{\ell m}(p)Y_{\ell' m'}(p) H(|\theta -\pi/2|-b)\\
&= \frac{N_{\mathrm{pix}}}{\sigma^2}\sum_p \Tilde{Y}_{\ell m}\Tilde{Y}_{\ell' m'} e^{-i(m-m')\phi} H(|\theta -\pi/2|-b).
\end{align}
Here, we used that $Y_{\ell m}(p) = \tilde{Y}_{\ell m} e^{im\phi}$ where it is implied that $\tilde{Y}_{\ell m}=\tilde{Y}_{\ell m}(\theta)$, and $\theta$ and $\phi$ are functions of $p$.

We now take the limit where there is a large amount of pixels with equal surface area each. In this limit, we change the sum into an integration where we account for the number of pixels per area element
\begin{equation}
\sum_p \rightarrow \frac{N_{\mathrm{pix}}}{4\pi}\int d\Omega  = \frac{N_{\mathrm{pix}}}{4\pi}\int_{0}^{2\pi} d\phi \int_{0}^{\pi} d\theta \sin(\theta).
\end{equation}
This gives us in spherical coordinates
\begin{align}
\nonumber
\left(N^{-1}\right)_{\ell m \ell' m'} &= \frac{N_{\mathrm{pix}}^2}{4\pi \sigma^2}\int_{0}^{2\pi} d\phi \int_{0}^{\pi} d\theta \sin(\theta)\tilde{Y}_{\ell m}  \tilde{Y}_{\ell' m'}  e^{-i(m-m')\phi}
\\
\nonumber
&\cdot H(|\theta -\pi/2|-b)\\
\nonumber
&= \frac{N_{\mathrm{pix}}^2}{2\sigma^2} \delta_{mm'}\int_{0}^{\pi} d\theta \sin(\theta)\tilde{Y}_{\ell m}  \tilde{Y}_{\ell' m'} H(|\theta -\pi/2|-b)\\
\nonumber
&= \frac{N_{\mathrm{pix}}^2}{2\sigma^2} \delta_{mm'}\\
&\cdot \left(\int_{0}^{\pi/2-b} d\theta \sin(\theta) \tilde{Y}_{\ell m}  \tilde{Y}_{\ell' m'}+\int_{\pi/2+b}^{\pi} d\theta \sin(\theta)\tilde{Y}_{\ell m}  \tilde{Y}_{\ell' m'}\right).
\end{align}
Writing the above equation in terms of Legendre polynomials $P_{\ell m}(\cos(\theta)))$, we have
$\Tilde{Y}_{\ell m} = \Delta_{\ell m}P_{\ell m}(\cos(\theta)))$, where ${\Delta_{\ell m}=(-1)^m \sqrt{\frac{2\ell+1}{4\pi}\frac{(\ell - m)!}{(\ell+m)!}}}$.
 Writing $x=\cos(\theta)$, we know that Legendre polynomials $P_{\ell m}(x)$ are either symmetric or antisymmetric in $x\rightarrow-x$. $P_{\ell m}(x)$ is symmetric in $x \rightarrow -x$ when $\ell+m$ = even and antisymmetric when $\ell+m$ = odd. Since $m=m'$, we note that the two integrals cancel each other if $\ell+\ell' =$ odd. We, therefore, only get non-zero elements when $\ell + \ell' =$ even, for which the two integrals are equal. Hence, for $\ell + \ell'=$ even, we get
\begin{align}
\nonumber
\left(N^{-1}\right)_{\ell m \ell' m'} &= \frac{N_{\mathrm{pix}}^2}{\sigma^2} \delta_{mm'}\int_{0}^{\pi/2-b} d\theta \sin(\theta)\Tilde{Y}_{\ell m}(\theta)\Tilde{Y}_{\ell' m'}(\theta)\\
\label{eq:finished_n_inv}
&=\frac{N_{\mathrm{pix}}^2}{\sigma^2} \delta_{mm'}\int_{\sin(b)}^{1} dx \, \Tilde{Y}_{\ell m}(\arccos(x)) \Tilde{Y}_{\ell' m}(\arccos(x)).
\end{align}
This integral can be solved numerically by gridding $x$, and the spherical harmonics where the phase factor is removed can be calculated in Python using a library like ...

The azimutally symmetric Galactic mask used for this work has a sky coverage of $f_{\mathrm{sky}} = 0.90$, giving $\sin(b) = 0.1$. Therefore, we require much fewer grid points for $x$ if we use the following identity for $\ell+\ell' = $ even:
\begin{align}
&\delta_{mm'} \int_{\sin(b)}^{1} dx \, \Tilde{Y}_{\ell m}(\arccos(x)) \Tilde{Y}_{\ell' m}(\arccos(x)) = \\
\frac{1}{4\pi}\delta_{\ell \ell'}\delta_{m m'} - &\delta_{mm'} \int_{0}^{\sin(b)} dx \, \Tilde{Y}_{\ell m}(\arccos(x)) \Tilde{Y}_{\ell' m}(\arccos(x)),
\end{align}
which comes from the orthonormality condition for spherical harmonics. We now only need to grid $x$ in the interval $0\leq x \leq \sin(b) = 0.1$, as opposed to the interval $0.9 \leq x \leq 1$.

Since $N^{-1}_{\ell m \ell' m'} \propto \delta_{m m'}$, we get simplified matrix expressions. Imagine multiplying the matrix $\boldsymbol{N}^{-1} = N^{-1}_{\ell m \ell' m'}$ with the vector $\boldsymbol{b} = b_{\ell m}$
\begin{align}
\boldsymbol{N}^{-1} \cdot \boldsymbol{b} &= \sum_{\ell' m'}\left(N^{-1}\right)_{\ell m \ell' m'}b_{\ell' m'} = \sum_{\ell'}\left(N^{-1}\right)_{\ell m \ell' m}b_{\ell' m}\\
&= \sum_{\ell' }\left(N^{-1}\right)^{(m)}_{\ell, \ell'}b^{(m)}_{\ell'}.
\end{align}
To solve the map-making equation, we get a matrix equation for each $0 \leq m \leq \ell_{\mathrm{max}}$. This gives us $\ell_{\textrm{max}}+1$ number of matrix equations where the dimensions of the matrices are maximally $\ell_{\textrm{max}} \times \ell_{\textrm{max}}$. This is numerically much quicker than inverting the full $(\ell_{\textrm{max}})^2 \times (\ell_{\textrm{max}})^2$ matrix once.


\end{document}
