\documentclass[twocolumn]{../common/aa}
%\documentclass[referee]{aa}

\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{txfonts}
\usepackage{color}
\usepackage{natbib}
\usepackage{float}
%\usepackage{stfloats}
\usepackage{dblfloatfix}
\usepackage{afterpage}
\usepackage{ifthen}
\usepackage[morefloats=12]{morefloats}
\usepackage{placeins}
\usepackage{multicol}
\bibpunct{(}{)}{;}{a}{}{,}
\usepackage[switch]{lineno}
\definecolor{linkcolor}{rgb}{0.6,0,0}
\definecolor{citecolor}{rgb}{0,0,0.75}
\definecolor{urlcolor}{rgb}{0.12,0.46,0.7}
\usepackage[breaklinks, colorlinks, urlcolor=urlcolor,
    linkcolor=linkcolor,citecolor=citecolor,pdfencoding=auto]{hyperref}
\hypersetup{linktocpage}
\usepackage{bold-extra}
\usepackage{xcolor}

%\usepackage[grid,
%  gridcolor=red!20,
%  subgridcolor=green!20,
%  gridunit=cm]{eso-pic}
%

\input{../common/Planck}

\def\WMAP{\emph{WMAP}}
\def\WMAPnine{\emph{WMAP9}}
\def\COBE{\emph{COBE}}
\def\wmap{\emph{WMAP}}
\def\planck{\emph{Planck}}
\def\Planck{\emph{Planck}}
\def\LCDM{$\Lambda$CDM}
\def\ffp{FFP6}
\def\unionmask{U73}
\def\nside{N_{\mathrm{side}}}

\def\healpix{\texttt{HEALPix}}
\def\commander{\texttt{Commander}}
\def\commanderone{\texttt{Commander1}}
\def\commandertwo{\texttt{Commander2}}
\def\commanderthree{\texttt{Commander3}}
\def\ruler{\texttt{Ruler}}
\def\comrul{\texttt{Commander-Ruler}}
\def\CR{\texttt{C-R}}
\def\nilc{\texttt{NILC}}
\def\gnilc{\texttt{GNILC}}
\def\sevem{\texttt{SEVEM}}
\def\smica{\texttt{SMICA}}
\def\CamSpec{\texttt{CamSpec}}
\def\Plik{\texttt{Plik}}
\def\XFaster{\texttt{XFaster}}
\def\sroll2{\texttt{SRoll2}}

\renewcommand{\d}[0]{\vec{d}}
\renewcommand{\t}[0]{\vec{t}}
\newcommand{\A}[0]{\tens{A}}
\newcommand{\B}[0]{\tens{B}}
\renewcommand{\G}[0]{\tens{G}}
\renewcommand{\C}[0]{\tens{C}}
\newcommand{\Y}[0]{\tens{Y}}
\newcommand{\n}[0]{\vec{n}}
\newcommand{\red}[0]{\color{red}}
\newcommand{\green}[0]{\color{green}}
\newcommand{\s}[0]{\vec{s}}
\renewcommand{\a}[0]{\vec{a}}
\newcommand{\m}[0]{\vec{m}}
\newcommand{\f}[0]{\vec{f}}
\newcommand{\F}[0]{\tens{F}}
\newcommand{\T}[0]{\tens{T}}
\newcommand{\Cp}[0]{\tens{C}}
\renewcommand{\L}[0]{\tens{L}}
\newcommand{\g}[0]{\vec{g}}
\newcommand{\N}[0]{\tens{N}}
\newcommand{\M}[0]{\tens{M}}
\newcommand{\iN}[0]{\tens{N}^{-1}}
\newcommand{\iM}[0]{\tens{M}^{-1}}
\newcommand{\w}[0]{\vec{w}}
\renewcommand{\S}[0]{\tens{S}}
\renewcommand{\r}[0]{\vec{r}}
\renewcommand{\u}[0]{\vec{u}}
\newcommand{\q}[0]{\vec{q}}
\renewcommand{\v}[0]{\vec{v}}
\renewcommand{\P}[0]{\tens{P}}
\newcommand{\dt}[0]{d_t}
\newcommand{\di}[0]{d_i}
\newcommand{\nt}[0]{n_t}
\newcommand{\st}[0]{s_t}
\newcommand{\mt}[0]{m_t}
\newcommand{\ft}[0]{f_t}
\newcommand{\Te}[0]{T_{\rm e}}
\newcommand{\EM}[0]{\rm EM}
\newcommand{\mathsc}[1]{{\normalfont\textsc{#1}}}
\newcommand{\hi}{\ensuremath{\mathsc {Hi}}}
\newcommand{\bpbold}{\bfseries{\scshape{BeyondPlanck}}}
\newcommand{\BP}{\textsc{BeyondPlanck}}
\newcommand{\bp}{\textsc{BeyondPlanck}}
\newcommand{\cosmoglobe}{\textsc{Cosmoglobe}}
\newcommand{\Cosmoglobe}{\textsc{Cosmoglobe}}
\newcommand{\lfi}[0]{LFI}
\newcommand{\hfi}[0]{HFI}
\newcommand{\npipe}[0]{\texttt{NPIPE}}
\newcommand{\K}[0]{\textit K}
\newcommand{\Ka}[0]{\textit{Ka}}
\newcommand{\Q}[0]{\textit Q}
\newcommand{\V}[0]{\textit V}
\newcommand{\W}[0]{\textit W}
\newcommand{\e}{\mathrm e}
\newcommand{\cvar}{\ensuremath{c(\vartheta, \varphi, \psi)}}


\def\bC{\tens{C}}
\def\ba{\vec{a}}
\def\ncha{N_\mathrm{cha}}
\def\nfg{N_\mathrm{fg}}

\newcommand{\data}{\vec d}
\newcommand{\ncorr}{\vec n_\mathrm{corr}}
\newcommand{\Dbp}{\Delta_\mathrm{bp}}

%\modulolinenumbers[5]
%\linenumbers

\newcommand{\includegraphicsdpi}[3]{
    \pdfimageresolution=#1  % Change the dpi of images
    \includegraphics[#2]{#3}
    \pdfimageresolution=72  % Change it back to the default
}

\renewcommand{\topfraction}{1.0}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{1.0}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \setcounter{dbltopnumber}{2}    % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.04}	% allow minimal text w. figs
    %   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.9}	% require fuller float pages
	% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.9}	% require fuller float pages

\def\adj{^{\dagger}}
\def\tp{^{\rm T}}
\def\inv{^{-1}}
\def\lm{{\ell m}}

\begin{document}

\title{\bfseries{\Cosmoglobe: Towards likelihood-free CMB cosmological\\ parameter estimation with end-to-end error propagation}}
\input{authors.tex}
%\authorrunning{From BeyondPlanck to Cosmoglobe}
\authorrunning{Eskilt et al.}
\titlerunning{CMB parameter estimation}

\abstract{
  We implement support for likelihood-free cosmological parameter estimation algorithm, as originally proposed by Racine et al.\ (2016), in the world's first end-to-end Bayesian CMB Gibbs sampler called \commander, and quantify its computational efficiency and cost. For a semi-realistic simulation with noise levels similar to \Planck\ LFI 70\,GHz, a maximum multipole of $\ell_{\mathrm{max}}=1300$, and an accepted sky fraction of $f_{\mathrm{sky}}=0.85$, we find that more than 5000 Conjugate Gradient iterations are required to achieve sufficient CMB map accuracy, and that the typical Markov chain correlation length is $\sim$\,100 samples. The net effective cost per independent sample is $\sim$\,5000\,CPU-hrs, which --- when integrated into a full end-to-end Bayesian pipeline --- is to be compared with the sum of all lower-level processing costs. As a relevant example, processing all \Planck\ LFI and \WMAP\ channels within the \cosmoglobe\ Data Release 1 pipeline costs 812\,CPU-hrs, and the addition of the new parameter estimation step discussed here will therefore increase the overall analysis cost of that particular data set by a factor of six. Thus, although technically possible to run already in its current state, future work should aim to reduce the effective cost per independent sample by at least one order of magnitude to avoid excessive runtimes; we argue that this is possible through improved multi-grid preconditioners and/or derivative-based Markov chain sampling schemes. At the same time, we also note that both the Conjugate Gradient mapmaking cost and the Markov chain correlation length depend strongly on the signal-to-noise ratio of the data in question, and next-generation CMB $B$-mode satellite polarization measurements generally have a much lower signal-to-noise ratio than \Planck\ and \WMAP's temperature constraints. On the one hand, this work demonstrates the computational feasibility of true Bayesian cosmological parameter estimation with end-to-end error propagation for high-precision CMB experiments without likelihood approximations; on the other hand, it also highlights the need for additional optimizations before it is justified for full production-level analysis. 
}

\keywords{ISM: general -- Cosmology: observations, polarization,
    cosmic microwave background, diffuse radiation -- Galaxy:
    general}

\maketitle

%\hypersetup{linkcolor=black}
%\tableofcontents
%\hypersetup{linkcolor=red} 




\section{Introduction}
\label{sec:introduction}

High-precision cosmic microwave background (CMB) measurements provide today the strongest constraints on a wide range of key cosmological parameters \citep[e.g.,][]{planck2016-l06}. Traditionally, these constraints are derived by first compressing the information contained in the high-dimensional raw time-ordered data into pixelized sky maps and angular power spectra \citep[e.g.,][]{bennett2012,planck2016-l02,planck2016-l03}, and this compressed dataset is typically summarized in terms of a low-dimensional power spectrum-based likelihood from which cosmological parameters are derived through Markov Chain Monte Carlo (MCMC) sampling \citep{cosmomc,planck2016-l05}.

A key element in this pipeline procedure is the so-called likelihood function, $\mathcal{L}(C_{\ell})$, where $C_{\ell}$ denotes the angular CMB power spectrum. This function summarizes both the best-fit values of all power spectrum coefficients and their covariances and correlations. The accuracy of the resulting cosmological parameters, both in terms of best-fit values and their uncertainties, correspond directly to the accuracy of this likelihood function. As such, this function must account for both low-level instrumental effects (such as correlated noise and calibration uncertainties) and high-level data selection and component separation effects. In general, this function is neither factorizable nor Gaussian, and except for a few well-known special cases it has no analytical exact and closed form. Unsurprisingly, significant efforts have therefore been spent on establishing computationally efficient and accurate approximations. Perhaps the most well-known example of such is a standard multi-variate Gaussian distribution in units of $C_{\ell}$, with a covariance matrix tuned by end-to-end simulations. A second widely used case is that of a low-resolution multi-variate Gaussian defined in pixel space with a dense pixel-pixel noise covariance matrix. Yet other examples include the log-normal or Gaussian-plus-lognormal distributions, as well as various hybrid combinations of all of these. 

However, as the signal-to-noise ratios of modern CMB data sets continue to increase, the relative importance of systematic uncertainties grows, and these are often highly non-trivial to describe through low-dimensional and simplified likelihood approximations, both in terms of computational expense and accuracy. These difficulties were anticipated and understood more than two decades ago, and an alternative and ground-breaking end-to-end Bayesian approach was proposed independently by \citet{jewell2004} and \citet{wandelt2004}. The key aspect in this framework is global analysis, in which all aspects of the data are modelled and fitted simultaneously, as opposed to compartmentalized as is typically done in traditional CMB pipelines. Technically speaking, this is implemented in the form of an MCMC sampler in a process that is statistically analogous to the very last cosmological parameter estimation step in a traditional pipeline, such as CosmoMC \citep{cosmomc} or Cobaya \citep{Torrado:2020dgo}, with the one fundamental difference that all parameters in the entire analysis pipeline are sampled over jointly within the MCMC sampler. Thus, while a standard cosmological parameter code typically handles a few tens, or perhaps a hundred, free parameters, the global approach handles billions of free parameters. In practice, the only practical method of dealing with such a large number of parameters of different types proposed to date is through Gibbs sampling, which draws samples from a joint distribution by iterating over all relevant conditional distributions.

At a strictly principled level, a global approach is obviously preferable, simply because better results are in general obtained by fitting correlated parameters jointly rather than separately. However, and equally clearly, this approach is also organizationally and technically more complicated to implement in practice, simply because all aspects of the analysis has to be accounted for simultaneously within one framework. The most advanced effort with this aim published to date is \commanderthree\ \citet{bp05}, which is the world's first end-to-end CMB Gibbs sampler developed by the \BP\ collaboration to re-analyze the \Planck\ Low Frequency Instrument (LFI) observations. This work has subsequently been superseded by the \cosmoglobe\ project\footnote{\url{https://cosmoglobe.uio.no}}, which is a community-wide Open Science collaboration that ultimately aims to perform the same type of analysis for all available state-of-the-art large-scale radio, microwave, and sub-millimeter experiments, and use this to derive one global model of the astrophysical sky. The first \cosmoglobe\ Data Release (DR1) took place in March 2023 \citep{watts2023_dr1}, and included the first joint end-to-end analysis of both \Planck\ LFI and the Wilkinson Microwave Anisotropy Probe (WMAP; \citealp{bennett2012}), resulting in lower systematic residuals in both experiments.

While \commanderthree\ arguably represents the most advanced integrated CMB analysis pipeline in the literature, one key component is still missing before one can claim to perform true end-to-end CMB analysis, and that is in fact cosmological parameter estimation; right now, the highest-level outputs from \commanderthree\ is a set of angular power spectrum samples \citep{bp11}, from which cosmological parameters may be derived using traditional methods \citep{bp12}, much like in a traditional compartmentalized pipeline. The main goal of the current paper is to implement that missing step, to quantify its overall computational cost, and to identify potential bottlenecks.

The numerical challenges involved in implementing likelihood-free cosmological parameter estimation in Gibbs sampling was originally explored and partially resolved by \citet{jewell:2009}. In short, the fundamental difficulty lies in the fact that a strict Gibbs sampler only allows parameter moves in directions that are orthogonal, and this leads to a long Markov chain correlation length in the low signal-to-noise regime, where the CMB sky signal and the theoretical model are nearly perfectly degenerate. To circumvent this problem, \citet{racine:2016} therefore proposed a joint sampling step for both the sky signal and theory model that allow these to move relatively quickly throughout the full parameter space. However, they only demonstrated the method with a proof-of-concept and stand-alone Python implementation using an ideal case. In the current paper, we implement the same method in \commanderthree, and apply it to simulations with realistic noise and sky coverages. From a formal point-of-view, this work therefore completes the end-to-end aspect of the \commanderthree\ code. 

The rest of the paper is organized as follows. In Sect.~\ref{sec:methods} we provide a brief review of both the \cosmoglobe\ Gibbs sampler method in general and the specific cosmological parameter sampler described by \citet{racine:2016}. In Sect.~\ref{sec:results}, we validate our implementation by comparing it to two special cases, namely Cobaya coupled to a uniform and full-sky case and a special-purpose Python sampler for a case with uniform noise but a constant latitude Galactic mask. In the same section, we also characterize the computational costs of the new sampling step for various data configurations. Finally, we summarize and conclude in Sect.~\ref{sec:conclusions}.

\section{Algorithms}
\label{sec:methods}

We start by briefly reviewing the Bayesian framework for global end-to-end analysis as described by \citet{bp01,watts2023_dr1} and the cosmological parameter estimation algorithm proposed by \citet{racine:2016}, and discuss how these may be combined in \commanderthree\ \citep{bp03}.

\subsection{Bayesian end-to-end CMB analysis}

The main algorithmic goal of the \cosmoglobe\ framework is to derive a numerical representation of the full posterior distribution $P(\omega|\d)$, where $\omega$ is the set of all free parameters and $\d$ represents all available data. In this notation, $\omega$ simultaneously accounts for instrumental, astrophysical and cosmological parameters, and are typically explicitly defined by writing down a signal model, for instance taking the form
\begin{equation}
    \label{eq:data_model}
    \d = \A \a + \n,
\end{equation}
where $\d$ is the data vector; $A$ represents some set of linear operations; $\a$ denotes some set of amplitude parameters; and $\n$ is instrumental noise. In practice, one typically assumes that the noise is Gaussian distributed with a covariance matrix $\N$, and the likelihood may therefore be written as
\begin{equation}
  \mathcal{L}(\omega) = P(\d|\omega) \propto e^{-\frac{1}{2}\left(\d-\s(\omega)\right)^t\N^{-1}\left(\d-\s(\omega)\right)}.
\end{equation}
The full posterior distribution reads $P(\omega|\d) \propto \mathcal{L}(\omega)P(\omega)$, where $P(\omega)$ represents some set of user-defined priors.

As a concrete example, \cosmoglobe\ Data Release 1 \citep{watts2023_dr1} adopted the following data model to describe \Planck\ LFI and \WMAP,
\begin{equation}
	\label{eq:model}
	\d =\G\P\B\M\boldsymbol \a+ \s^\mathrm{orb}
	+\s^\mathrm{fsl} + \s^\mathrm{inst}+ \n^\mathrm{corr}+\n^\mathrm w,
\end{equation}
where $\mathsf G$ is a time-dependent gain factor; $\mathsf P$ is a pointing matrix;
$\B$ denotes instrumental beam convolution; $\mathsf M$ is a mixing matrix that describes the amplitude of a given sky component at a given frequency; $\a$ describes the amplitude of each component at each point in the sky; $\s^\mathrm{orb}$ is the orbital CMB dipole; $\s^\mathrm{fsl}$ is a far sidelobe contribution; $\s^\mathrm{inst}$ is an instrument-specific correction term; and $\n^\mathrm{corr}$ denotes correlated noise. This expression directly connects important instrumental effects (e.g., gains, beams and correlated noise) with the astrophysical sky (e.g., $\M$ and $\a$), and provides a well-defined model for the data at the lowest level; this model is what enables global modelling.

For our purposes, the most important sky signal component is the CMB anisotropy field, $\s_{\mathrm{CMB}}$. The covariance of this component reads $\S = \langle \s_{\mathrm{CMB}}^T \s_{\mathrm{CMB}}\rangle$. Under the assumption of a statistically isotropic universe, this matrix is diagonal in harmonic space, $\langle s_{\ell m} s^*_{\ell' m'}\rangle = C_{\ell m}(\theta) \delta_{\ell\ell'}\delta_{mm'}$, where $s(\hat{n}) = \sum_{\ell,m} s_{\ell m} Y_{\ell m}(\hat{n})$ is the usual spherical harmonics expansion and $C_{\ell}$ is called the angular power spectrum. This power spectrum depends directly on a small set of cosmological parameters, $\theta$, and may for most universe models be computed efficiently by Boltzmann solvers such as CAMB or CLASS.

With this notation, the goal is now to compute $P(\omega|\d)$, where $\omega = \{\G, \n_{\mathrm{corr}}, \M, \a, \theta, \ldots\}$. Unfortunately, directly evaluating or sampling from this distribution is unfeasible. In practice, we therefore resort to Markov Chain Monte Carlo sampling in general, and Gibbs sampling in particular, which is a well-established method for sampling from complicated multivariate distribution by iterating over all conditional distributions. For the data model described above, this process may be described schematically in the form of a Gibbs chain, 
  \begin{alignat}{9}
    \label{eq:gain_samp_dist}\G &\,\leftarrow          P(\G&\,               \mid \data, &\,\phantom{\G,} &\,\ncorr,&\,\M, &\,\a, &\,\theta)\\
    \label{eq:ncorr_samp_dist} \ncorr &\,\leftarrow    P(\ncorr&\,        \mid \data, &\,\G, &\,\phantom{\ncorr,}  &\,\M, &\,\a, &\,\theta)\\
    \label{eq:beta_samp}\M &\,\leftarrow                     P(\M &\, \mid \data, &\,\G, &\,\ncorr, &\,\phantom{\M}, &\,\a, &\,\theta)\\
    \a &\,\leftarrow                                   P(\a&\,            \mid \data, &\,\G, &\,\ncorr, &\,\M, &\,\phantom{\a,} &\,\theta)\\
    \theta &\,\leftarrow                             P(\theta &\,         \mid \data, &\,\G, &\,\ncorr, &\,\M, &\,\a,&\,\phantom{\theta})\label{eq:param_samp},\\\vspace*{-4mm}
     &&\vdots                             & & & & & & \nonumber
    \end{alignat}
  where $\leftarrow$ indicates replacing the parameter on the left-hand side with a random sample from the distribution on the right-hand side.

As described by \citet{bp01,watts2023_dr1}, this process has now been implemented in \commanderthree\ by the \BP\ and \Cosmoglobe collaborations, and demonstrated to work very well in practice with \Planck\ LFI and \WMAP. However, in the last step of the existing code, the last step only supports power spectrum estimation, i.e., $\theta = C_{\ell}$; the goal of the current paper is to replace that step with actual cosmological parameter estimation, in which $\theta$ takes on the usual form of the dark matter density $\Omega_\mathrm{c}$, the Hubble constant $H_0$, the spectral index of scalar perturbation $n_\mathrm{s}$ etc.

\subsection{Joint sampling of CMB sky signal and cosmological parameters}

The challenge of sampling cosmological parameters efficiently within an end-to-end Gibbs sampling framework has been addressed in the literature for almost two decades, starting with \citet{jewell2004} and \citet{wandelt2004}. As pointed out by \citet{eriksen:2004}, a major difficulty regarding this method is a very long correlation length in the low signal-to-noise regime, i.e., at high multipoles. Intuitively, the origin of this problem lies in the fundamental Gibbs sampling algorithm itself, namely that it only allows for parameter variations parallel or orthogonal to the coordinate axes of each parameter in question, and not diagonal moves. For highly degenerate distributions, this makes it very expensive to move from tail of the distribution to the other. Another way to visualize this is simply by noting that the Markov chain step size in a pure Gibbs sampling is given by CMB cosmic variance alone, while the full posterior distribution width is defined by both signal and noise; the Gibbs sampler therefore moves quickly in the high signal-to-noise regime, but slowly in the low signal-to-noise regime. A solution to this problem was proposed by \citet{jewell:2009}, who introduced a joint CMB sky signal and spectral parameter move. This idea was further refined by \citet{racine:2016}, who noted that faster convergence could be obtained by only re-scaling the low signal-to-noise multipoles. In the following, we give a brief summary of these ideas, both standard Gibbs sampling and joint sampling.

\subsubsection{Standard CMB Gibbs sampling}
\label{sec:gibbs}

Starting with the standard Gibbs sampling algorithm \citep{jewell2004,wandelt2004}, we first note that as far as cosmological parameter estimation with CMB data is concerned, the only directly relevant quantities in $\omega$ are the CMB map and the set of cosmological parameters, $\a_{\textrm{CMB}}$ (which we for brevity will denote only $\a$ in this section) and $\theta$. All other parameters in the end-to-end model in Eq.~\eqref{eq:data_model} only serve to produce the cleanest possible estimate of $\a$, and to propagate the corresponding uncertainties. We therefore for the moment consider the following greatly simplified data model,
\begin{equation}
  \r = \B\a + \n,
\end{equation}
where $\r$ now represents a foreground-cleaned CMB map (or residual) obtained by subtracting all other nuisance terms from the raw data, and our goal is to estimate cosmological parameters from this map. Under the assumption that the CMB is a Gaussian random field, this could in principle be done simply by mapping out the exact marginal posterior distribution by brute-force,
\begin{equation}
  P(\theta | \r) \propto \mathcal{L}(\theta)P(\theta) \propto \frac{e^{-\frac12 \r^T (\B^T \S(\theta) \B + \N)^{-1}\r}}{\sqrt{\left|\B^T \S(\theta) \B + \N\right|}},
  \label{eq:exact_posterior}
\end{equation}
were $\B^T \S(\theta) \B$ is the beam-convolved CMB signal covariance matrix and $\N$ is the corresponding noise covariance matrix, and we have assumed uniform priors, $P(\theta) = 1$. This, however, becomes intractable for modern CMB experiments as the computational expense scales as $\mathcal{O}(N_p^3)$, where $N_p$ is the number of pixels, and modern CMB experiments typically produce maps with many millions of pixels. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/s_hat_f_hat.pdf}
	\caption{\label{fig:sky_map}Example of a constrained CMB sky map realization produced during traditional Gibbs sampling. From top to bottom, the three panels show the mean field map ($\hat{\a}$), the fluctuation map ($\hat{\f}$), and the full constrained realization ($\a = \hat{\a}+\hat{\f}$). This example is generated with the data configuration discussed in Sect.~\ref{sec:results} corresponding to \Planck\ LFI 70\,GHz noise properties and a realistic Galactic mask.}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/sigma_ell.pdf}
	\caption{\label{fig:sigma_ell}Angular power spectra for each of the constrained realization maps shown in Fig.~\ref{fig:sky_map}. The $\Lambda$CDM cosmological model corresponding to $\theta$ is shown as a dashed black line, while the colored curves show (from dark to light) spectra for the fluctuation map, the mean field map, and the full constrained realization.}
\end{figure}

In the special (and unrealistic) case of uniform noise and no Galactic mask, this is likelihood can be simplified in harmonic space, in which it does become computationally tractable. Let us define the observed angular CMB power spectrum (including beam convolution and noise) to be $\hat{C}^{\mathrm{o}}_{\ell} = \frac{1}{2\ell+1}\sum_m r_{\ell m}r^*_{\ell m}$, and the corresponding white noise spectrum to be defined by $\N_{\ell m \ell'm'} = N_\ell \delta_{\ell \ell'}\delta_{mm'}$. In that case, the likelihood in Eq.~\eqref{eq:exact_posterior} for temperature-only observations simplifies to
{\small
\begin{equation}
  \ln P(\theta | \r) = \sum_{\ell} -\frac{2\ell+1}{2} \bigg[\frac{\hat{C}^{\mathrm{o}}_{\ell}}{b_\ell^2 C_{\ell}(\theta) + N_\ell}-\ln \left(\frac{\hat{C}^{\mathrm{o}}_{\ell}}{b_\ell^2 C_{\ell}(\theta) + N_\ell} \right) \bigg],
  \label{eq:exact_harm}
\end{equation}}
where $b_\ell$ is the beam response function and we have removed constant terms; for a similar expression for polarization, see, e.g., \citet{larson:2006,Hamimeche:2008ai}. This expression is used later to validate the production code.

While it is difficult to evaluate Eq.~\eqref{eq:exact_posterior} directly, it is in fact possible to sample from it using Gibbs sampling \citep{jewell2004,wandelt2004}. First, we note that the joint posterior distribution $P(\theta, \a | \r)$ can be written as
\begin{align}
    \nonumber
    P(\theta, \a | \r) &= \frac{P(\theta, \a, \r)}{P(\r)} = P(\r | \a)P(\a| \theta)\frac{P(\theta)}{P(\r)}\\
    \label{eq:joint-posterior}
    &= \frac{e^{-\frac12 \left(\r-\B\a \right)^T \N^{-1}\left(\r-\B\a \right)}}{\sqrt{\left|\boldsymbol{N}\right|}}
    \frac{e^{-\frac12 \a^T \S^{-1}\a}}{\sqrt{\left|\boldsymbol{S}\right|}}\frac{P(\boldsymbol{\theta})}{P(\boldsymbol{d})},
\end{align}
and drawing samples from this distribution can be achieved through Gibbs sampling,
\begin{align}
    \label{eq:a-gibbs}
    \a^{i+1} &\leftarrow P(\a | \theta^{i}, \r),\\
    \label{eq:theta-gibbs}
    \theta^{i+1} &\leftarrow P(\theta | \a^{i+1}, \r).
\end{align}
With these joint samples in hand, a numerical representation of $P(\theta | \r)$ can be obtained simply by marginalizing over $\a$, which is equivalent to simply making histograms of the $\theta$ samples.

For this algorithm to work, we need to be able to sample from each of the two conditional distributions in Eqs.~\eqref{eq:a-gibbs} and \eqref{eq:theta-gibbs}. Starting with the cosmological parameter distribution in Eq.~\ref{eq:theta-gibbs}, we note that the relevant distribution is in fact identical to that shown in Eq.~\ref{eq:exact_harm}, and this can be coupled to a standard Boltzmann solver and Metropolis-Hastings sampler, similar to CAMB and CosmoMC. We will return to this step in the next section.

Next, for the amplitude distribution in Eq.~\eqref{eq:a-gibbs} one can show by ``completing the square'' of $\a$ in Eq.~\eqref{eq:joint-posterior} that 
\begin{equation}
    P(\a | \theta, \r) \propto e^{-\frac12 \left(\a - \hat{\a}\right)^T \left(\S^{-1} + \B^T\N^{-1}\B\right) \left(\a - {\hat{\a}}\right)},
\end{equation}
where we have defined the so-called mean field map,
\begin{equation}
\label{eq:mean-field-map}
\hat{\a} \equiv \left[\S^{-1} + \B^t \N^{-1}\B \right]^{-1} \B^t \N^{-1} \r.
\end{equation}
This is a multivariate Gaussian distribution with mean $\hat{\a}$ and a covariance matrix $\left[\S^{-1} + \B^t \N^{-1}\B \right]^{-1}$ which can be sampled from solving the following equation by Conjugate Gradients \citep{shewchuk:1994,seljebotn:2019},
\begin{equation}
    \label{eq:mapmakingeq}
    \left[\S^{-1} + \B^t \N^{-1}\B \right]\a = \B \N^{-1} \r + \S^{-\frac{1}{2}}\w_0 +\B^t \N^{-\frac{1}{2}}\w_1,
\end{equation}
where $\w_0$ and $\w_1$ are randomly drawn Gaussian maps with unit variance and zero mean. The resulting sample is typically referred to as a ``constrained realization''.

For the purposes of the next section, it is useful to decompose the full constrained realization into two separate components, namely the mean field map defined by Eq.~\eqref{eq:mean-field-map} and the fluctuation map, 
\begin{equation}
\label{eq:fluc-map}
\hat{\f} \equiv \left[\S^{-1} + \B^t \N^{-1}\B \right]^{-1} \left(\S^{-\frac{1}{2}}\w_0 +\B^T \N^{-\frac{1}{2}}\w_1 \right)
\end{equation}
such that $\a = \hat{\a} + \hat{\f}$. For a data configuration with uniform noise and full-sky temperature data, these equations can be solved exactly in spherical harmonics space, fully analogously to Eq.~\eqref{eq:exact_harm},
\begin{align}
    \label{eq:hat_s_approx}
    \hat{a}_{\ell m} &= r_{\ell m}\frac{b_{\ell}C_{\ell}}{N_\ell + b_{\ell}^2C_{\ell}},\\
    \label{eq:hat_f_approx}
    \hat{f}_{\ell m} &= w_{0\ell m}\frac{N_{\ell}\sqrt{C_{\ell}}}{N_\ell + b_{\ell}^2C_{\ell}}+w_{1\ell m}\frac{\sqrt{N_{\ell}}b_{\ell}C_\ell}{N_\ell + b_{\ell}^2C_{\ell}}.
\end{align}
As summarized in Appendix~A, these equations can also be solved exactly for a constant latitude mask with isotropic noise, and we have made available a Python script that performs these calculations in order to validate the main \commanderthree\ code discussed below.

To build intuition, these maps are illustrated for one random sample with semi-realistic instrument characteristics in Fig.~\ref{fig:sky_map}. The top panel shows the mean field map (or ``Wiener filter map''); this map summarizes all significant information in $\r$, and each mode is weighted according to its own specific signal-to-noise ratio taking into account both instrumental noise and masking. As a result, all small scales in the Galactic plane are suppressed, and only the largest scales are retained, which are constrained by the high-latitude observations in conjunction with the assumptions of both Gaussianity and statistical isotropy. The second panel shows the fluctuation map, which essentially serves to replace the lost power in $\hat{\a}$ with a completely random fluctuation. The sum of the two terms, shown in the bottom panel, represents one possible full-sky realization that is consistent with both the data and the assumed cosmological parameters, $\theta$. If we were to produce a second such constrained realization, it would look almost identical at high Galactic latitudes, where the data are highly constraining, while the Galactic plane would exhibit different structure on small and intermediate scales.

A corresponding comparison of angular power spectra are shown in Fig.~\ref{fig:sigma_ell}. The relative signal-to-noise ratio of each multipole moment may in this figure be seen as the ratio between the intermediate and dark colored lines. In temperature, this value is higher than unity up to $\ell\lesssim700$, while for $EE$ polarization it is less than unity everywhere except at the very lowest multipoles. A closely related way to interpret this figure is that the intermediate curve shows the total information content of the data. Thus, for the $BB$ spectrum there is no visually relevant information present at all, and the entire constrained realization is entirely defined by the fluctuation map, which in terms of given by the assumed cosmological model, $\theta$. This is explains why the standard Gibbs sampler works well in the high signal-to-noise regime, but poorly in the low signal-to-noise limit: When we have high instrumental noise, $\boldsymbol{N} \rightarrow \infty$, then we have no information of $a_{\ell m}$ from the data $r_{\ell m}$, and the total sky sample only becomes only a realization of the assumed variance, $C_{\ell}$, i.e. $a_{\ell m} \rightarrow \hat{f}_{\ell m} = \sqrt{C_{\ell}} w_{0\ell m}$. However, since we have no information of the mean field map $\hat{a}_{\ell m} \rightarrow 0$, we also do not know $C_\ell$. Hence, we get a strong degeneracy between the $C_\ell$ and $a_{\ell m}$. Gibbs sampling is well-known to perform poorly for strongly correlated parameters, as one cannot move diagonally through the parameter space from one end of the joint posterior distribution to the other. 


\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/parameter_degeneracy.jpg}
	\caption{\label{fig:illustration}Schematic comparison of the standard Gibbs sampler (top row) and the joint sampler of \citet{racine:2016} (bottom row). Left and right columns show high and low signal-to-noise regimes, respectively. The Gibbs sampler perform poorly in the low signal-to-noise regime as it requires a large number of samples to explore the posterior distribution. The joint sampler in the lower panels performs well in both regimes as it allows the next sample to move diagonally in the $\{\a, \theta\}$ parameter space.}
\end{figure}

\begin{equation}
  L_A=L_{A_1} \, L_{A_2} \, \beta \, L_B
\end{equation}

\subsubsection{Joint sky signal and cosmological parameter sampling}
\label{sec:joint-sampling}

The joint sampler proposed by \citet{jewell:2009} and \citet{racine:2016} is designed to establish an efficient sampling algorithm for the degeneracy between $\a$ and $\theta$ in the low signal-to-noise regime. Mathematically, this algorithm is a standard Metropolis-Hastings sampler with a Gaussian transition rule for $\theta$,
\begin{equation}
w(\theta |\theta^i) = e^{-\frac12 \left(\theta - \theta^i \right)^T \C_{\theta}^{-1}\left(\theta - \theta^i \right)},
\end{equation}
where $\C_{\theta}$ is a user-specified (and pre-tuned) proposal covariance matrix, followed by a deterministic re-scaling of the fluctuation map in the signal amplitude map,
\begin{equation}
    a_{\ell m}^{i+1} = \hat{a}_{\ell m}^{i+1} + \left(\C^{i+1}_{\ell}\right)^{1/2}\left(\C^{i}_{\ell}\right)^{-1/2} \hat{f}_{\ell m}^{i}.
\end{equation}
As shown by \citet{racine:2016}, the acceptance rate for this joint move is
\begin{equation}
    \label{eq:acceptance-rate}
    A = \mathrm{min}\left[1, \frac{\pi(\theta^{i+1})}{\pi(\theta^i)} \frac{P(\theta^{i+1})}{P(\theta^i)} \right],
\end{equation}
where $P(\theta)$ is the prior on $\theta$ and
\begin{align}
    \nonumber
    \pi(\theta^{i}) = \mathrm{exp}\bigg[&-\frac12 \left(\r-\B\hat{\a}^i\right)^T \N^{-1}\left(\r-\B\hat{\a}^i\right)\\
    &-\frac12\a^{i,T} \S^{i, -1}\hat{\a}^i -\frac12 \hat{\f}^{i, T}\B^T\N^{-1} \B\hat{\f}^i\bigg],
\end{align}
where we for brevity have omitted the $\theta^i$ dependence of $\hat{\a}^i$, $\hat{\f}^i$ and $\S^i$. 

We note that the acceptance rate uses the scaled fluctuation term, $f_{\ell m}^{\textrm{scaled},\, i+1}$, for sample $i+1$ instead of the calculated fluctuation term in Eq.~\eqref{eq:mapmakingeq}. Hence, we only need to calculate $f_{\ell m}^{i+1}$ from this equation if the sample is accepted. This allows us to save roughly half the computational time of discarded samples as compared to accepted samples.

The full algorithm can now be summarized in terms of the following steps:
\begin{enumerate}
    \item We start with an initial guess of the cosmological parameters, $\theta^0$. From that, we calculate $\S^0$ with CAMB; the mean field map, $\hat{\a}^0$ from Eq.~\ref{eq:mean-field-map}; and the fluctuation map, $\hat{\f}^0$, from Eq.~\eqref{eq:fluc-map}.
    \item We then draw a new cosmological parameter sample from $w(\theta|\theta^i)$, and re-evaluate $\S^{i+1}$, $\hat{\a}^{i+1}$, and the scaled fluctuation term $\f_{\ell m}^{\textrm{scaled},\, i+1}$.
    \item We calculate the acceptance term of Eq.~\eqref{eq:acceptance-rate}, and accept or reject according to the regular Metropolis-Hasting rule.
    \item If the sample is accepted, then we calculate $\f^{i+1}$. This term will in the next iteration become $f^{i}$ which will appear in the acceptance probability and the equation for $f^{\textrm{scaled},\, i+1}$. 
    \item Iterate 2--4.
\end{enumerate}

The intuition behind this joint move is illustrated in Fig.~\ref{fig:illustration}. The top panel shows the standard Gibbs sampling algorithm, in which only parameter moves parallel to the axes are allowed. This works well in the high signal-to-noise regime (left column), where the joint distribution is close to symmetric. In the low signal-to-noise regime, however, the distribution is strongly tilted, because of the tight coupling between $\f$ and $\theta$ discussed at the end of Sect.~\ref{sec:gibbs}, and many orthogonal moves are required in order to move from one tail of the distribution to the other. With the joint algorithm, however, steps are non-orthogonal, with a slope defined exactly by the local signal-to-noise ratio of the mode in question. The net result is a much faster exploration of the full distribution.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/dist_posterior_no_mask.pdf}
	\caption{\label{fig:nomask}Comparison of cosmological parameter posterior distributions derived with \commanderthree\ (blue), Python (red), and Cobaya (gray) for a simulation with uniform noise and full-sky coverage. The true input cosmological parameter values are indicated by vertical and horizontal dashed lines.}
\end{figure}


\section{Results}
\label{sec:results}

The main goal of this paper is to implement the joint sampling algorithm into \commanderthree\ and characterize its performance both in terms of accuracy and performance on simulated data. All \commander\ code used in the following is available as Open Source software in a GitHub repository\footnote{https://github.com/Cosmoglobe/Commander}. For the purposes of the current paper, we focus on a standard six-parameter $\Lambda$CDM model, and choose $\theta=(\Omega_{\textrm{b}}h^2, \Omega_{\textrm{CDM}}h^2, H_0, \tau, A_s, n_s)$ as our base parameters, where $\Omega_\mathrm{b}$ and $\Omega_\mathrm{CDM}$ are the current density of baryonic and cold dark matter; $H_0$ is the Hubble parameter (and $h$ is the normalized Hubble constant, $h=\frac{H_0}{100\,\mathrm{km s}^{-1} \mathrm{MPc}^{-1}}$); $\tau$ is the optical depth at reionization; and $A_s$ and $n_s$ are the amplitude and tilt of the scalar primordial power spectrum. We use \texttt{CAMB}\footnote{\url{https://github.com/cmbant/CAMB}} to evaluate all $\Lambda$CDM power spectra, $\C_{\ell}(\theta)$ \citep{Lewis:1999bs}. 

In the following, we will consider three different simulations, corresponding to different complexity and realism. The absolute noise level of all three cases are selected to match that of the \Planck\ LFI 70\,GHz channel, noting that a natural early target for this algorithm is a reanalysis of \cosmoglobe\ DR1. For the two first cases, the noise level is isotropic with a value matching the full-sky 70\,GHz mean, while it for the third case is given by one realization of the actual non-isotropic \BP\ 70\,GHz noise rms distribution.

The second difference between the three cases is their sky coverages. The first case considers full-sky data, for which the analytical posterior distribution solution is readily available through Eq.~\eqref{eq:exact_harm}. The second case implements a constant-latitude Galactic mask for which an analytical expression is also available, albeit more complicated than for the full sky; see Appendix~A for details. The third case uses the actual \cosmoglobe\ DR1 confidence mask, for which no analytical expression is available. 

To validate the main \commanderthree\ results, we have developed a special-purpose Python program\footnote{Publicly available at \url{https://github.com/LilleJohs/.}} that implements the same joint sampling algorithm described above for the two first cases, and this serves essentially as a basic bug-check with respect to the Fortran implementation. In addition, we have also implemented a uniform, full-sky likelihood module for Cobaya \citep{Torrado:2020dgo}, an industry-standard cosmological parameter Metropolis sampler, and this serves as an end-to-end check on the entire code, including integration with CAMB.

\subsection{Accuracy validation}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/subplots_posterior_10_mask.pdf}
	\caption{\label{fig:mask10}Comparison of marginal cosmological parameter posterior distributions derived with \commanderthree\ and Python for a simulation with uniform noise and a 10\,\% constant latitude mask. The total number of independent samples produced by the two codes are $\mathcal{O}(10^2)$, accounting for the Markov chain correlation length of the algorithm, and the Monte Carlo uncertainty due to a finite number of samples is therefore significant.}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/realistic_chain.pdf}
	\caption{\label{fig:traceplot}Trace plot of the cosmological parameters for a semi-realistic case based on the \Planck\ LFI $70\,$GHz noise level and a realistic Galactic mask. The dashed horizontal lines show the true input values.}
\end{figure}


The first main goal of this paper is to demonstrate that the new \commanderthree\ implementation of the Racine et al.\ algorithm performs as expected with respect to accuracy. Thus, we start our discussion by considering the uniform and full-sky case discussed above. For each of the three available codes---\commanderthree, Python, and Cobaya---we produce 50\,000 samples, and show the full posterior distributions in Fig.~\ref{fig:nomask}. The agreement between all three codes are excel lent, and the overall variations are at most $\sim$\,0.1$\,\sigma$; this level of agreement is consistent with the number of samples and the corresponding Markov chain correlation lengths. 

Second, Fig.~\ref{fig:mask10} shows a similar comparison between the \commanderthree\ and Python implementation. The sky mask is in this case defined by $|b|<5.7^{\circ}$, which removes 10\,\% of the sky. Also in this case do we find good agreement between the two implementations, and the true input values (marked by vertical dashed lines) all fall well within the full posterior distributions. However, we do note that the introduction of a sky mask significantly increases the overall run time of the algorithm, since one now has to solve for the mean field map repeatedly at each step with Conjugate Gradients, rather than by brute-force matrix inversions. In total, we only produce $\sim$\,11000 and 17000 samples for each of the two codes for this exercise, at a total cost of $\mathcal{O}(10^5)$ CPU-hrs.

Finally, in Fig.~\ref{fig:traceplot} we show trace plots for each cosmological parameter for the realistic configuration as produced with \commanderthree, noting that none of the other codes are technically able to produce similar estimates. In this case, the computational cost is even higher, and we only produce a total of 1600 samples. Still, by comparing these traces with the true input values (marked as dashed horizontal lines), we do see that the resulting samples agree well with the expected values, both in terms of means and uncertainties. 

\subsection{Markov chain correlation length and computational costs}
\label{sec:resources}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/auto_correlation.pdf}
	\caption{\label{fig:autocorrelation}Markov chain auto-correlation functions for Cobaya (solid lines) and \commanderthree\ for a set of typical cosmological parameters. Dashed and dot-dashed lines show \commanderthree\ results for the full-sky case and the 10\,\% mask case, respectively. The horizontal gray dashed line indicates an auto-correlation of 0.1, which we use to define the overall correlation length.   }
\end{figure}

The second main goal of this paper is to quantify the computational costs involved in this algorithm, and identify potential bottlenecks that may be optimized through future work. The effective cost per independent Markov chain sample may be written as the product of the raw sampling cost per sample and the overall Markov chain correlation length. The latter of these may be quantified in terms of the auto-correlation function,
\begin{equation}
  \zeta_\theta(\Delta) \equiv \left<\frac{(\theta_i - \mu)}{\sigma}\frac{(\theta_{i+\Delta} - \mu)}{\sigma}\right>,
\end{equation}
where $\mu$ and $\sigma$ are the posterior mean and standard deviation computed from the Markov chain for $\theta$. We define the overall correlation length to be the first value of $\Delta$ for which $\zeta < 0.1$. This function is plotted for both Cobaya and \commanderthree\ in Fig.~\ref{fig:autocorrelation}; for the latter, dashed lines shows full-sky results and dot-dashed lines show results for a 10\,\% mask. Overall, we see that the \commanderthree\ correlation length is 50--60 for the full-sky case, increasing to 50--100 for the masked case. For comparison, Cobaya typically has a correlation of about 10.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/run_time.pdf}
	\caption{\label{fig:runtime}CPU-hours per accepted sample. This is compared to the 812 CPU-hours to generate one single end-to-end sample of \planck\ LFI and \wmap\ channels in red.}
\end{figure}

The basic computational cost per sample for \commanderthree\ is plotted as black points in Fig.~\ref{fig:runtime}. Here we see that the overall cost increases rapidly with decreasing sky coverage, starting at 0.2\,CPU-hrs for a full-sky sample to about 10,CPU-hrs for a 20,\,\% constant latitude mask and uniform noise. Likewise, allowing for a spatially varying noise distribution also increases the runtime, and for the realistic LFI 70\,GHz scanning pattern and 15\,\% mask, the cost is 60\,CPU-hrs/sample. Taking into account that the overall correlation length is $\sim$\,100, the effective cost is therefore about 6000\,CPU-hrs per independent sample; the gray dots in Fig.~\ref{fig:runtime} indicates this total cost for each configuration.

In order to interpret the implications of these costs, it is useful to compare with the overall cost of the full end-to-end processing pipeline within which this algorithm is designed to operate, and the most relevant comparison point is therefore the cost of the \cosmoglobe\ DR1 processing, which is 812\,CPU-hrs for the combination of \Planck\ LFI and \WMAP, marked as a red dot in Fig.~\ref{fig:runtime}. In its current state, we find that the new cosmological parameter step alone is thus about seven times more expensive than the full end-to-end processing. 

\section{Summary and discussion}
\label{sec:conclusions}

This paper had two main goals. The first goal was simply to implement the cosmological parameter sampling algorithm proposed by \citet{racine:2016} into the state-of-the-art \commanderthree\ CMB Gibbs sampler, and demonstrate that this new step works in a production environment. Based on a head-to-head comparison with both Cobaya and a stand-alone Python implementation demonstrates the fidelity of the method, we conclude that this goal has been successfully accomplished, and as such this work finally adds true ``end-to-end'' processing abilities to \commanderthree. 

The second goal was to measure the computational performance of the method, assess whether it is already suitable for large-scale production, and identify potential bottlenecks that should be optimized through future work. In this case, we find that the overall cost per independent Gibbs sample for a dataset similar to \cosmoglobe\ DR1 is about 6000\,CPU-hrs, which is a factor of seven higher than the total cost of all other steps in the end-to-end algorithm, including low-level calibration, mapmaking and component separation. Therefore, although the code is technically operational at the current time, we also conclude that it is still too expensive to be useful in a full-scale production environment. As a real-world comparison point, we note that the total processing wall-time for the \BP\ analysis was three months, and adding this new step would increase that to 24 months; while technically possible, this is clearly highly uncomfortable.

At the same time, this analysis has also revealed the underlying origin of these high expenses, and those can be divided into two parts. First, the overall cost per dependent Gibbs sample is dominated by the expense for solving for the mean field and fluctuation maps by Conjugate Gradients. In this respect, we note that the current implementation uses a standard diagonal preconditioner to solve these equations, as described by \citet{eriksen:2004}. However, after more than two decades of development, far more efficient preconditioners have been explored and described in the literature, including multi-resolution or multi-grid methods \citep[e.g.,][]{seljebotn:2013,seljebotn:2019}. Implementing one of these can potentially reduce the basic cost per sample by one or two orders of magnitude. The second issue is a long correlation length. In this respect, we note that the current algorithm is based on a very basic random-walk Metropolis sampler, which generally performs poorly for highly correlated parameters; in the cases considered here, the correlation between amplitude of scalar perturbations and the optical depth of reionization is a typical example of such a correlation. This suggests that the correlation length can be greatly reduced in several ways, for instance either by adding new data sets (such as weak gravitational lensing constraints) or by implementing a more sophisticated sampling algorithm that uses derivative information, such as a Hamiltonian sampler. Overall, we consider it very likely that future work will be able to reduce the total cost by one or two orders of magnitude, as needed for production, and we hope that the current results may provide inspiration and motivation for experts in the field to join the effort.

Finally, before concluding this paper, we also note that a key application of the Bayesian end-to-end analysis framework as pioneered by \BP\ and \cosmoglobe\ is the analysis of next-generation CMB $B$-mode experiments, for instance LiteBIRD. In this case, it is worth noting that the polarization-based signal-to-noise ratio of the tensor-to-scalar ratio, $r$, is much lower than \Planck\ and \WMAP's temperature signal-to-noise ratio to the $\Lambda$CDM parameters, and both the CG cost and Markov chain correlation length are likely to much shorter than for the case considered in this paper. As such, it is conceivable that already the current method performs adequately for LiteBIRD, and this will be explored through future work.


\bibliographystyle{../common/aa}

\bibliography{../common/Planck_bib,../common/CG_bibliography}

\appendix

\section{Analytic expression for a constant latitude mask}
\label{sec:appendixA}


The goal of this appendix is to show how we can calculate $N_{\ell m \ell' m'}^{-1}$ analytically for a constant latitude mask and uniform noise, and how this expression makes the map making equation computationally fast to solve. The noise can be written as this in pixel space:
$$
\left(N^{-1} \right)_{pp'} = \frac{N_{\mathrm{pix}}}{\sigma^2} \delta_{pp'} H(|\theta(p) -\pi/2|-b).
$$
where $H$ is the Heavyside function, meaning that we mask every pixel $p$ where $|\theta(p) -\pi/2| < b$ for some latitude $b$ in radians. For a masked pixel, $\left(N^{-1} \right)_{pp}=0$ which means that $N_{pp} = \infty$ for that pixel, as we want.

Using that
$$
\left(N^{-1}\right)_{pp'} = \sum_{\ell m}\sum_{\ell' m'} \left(N^{-1}\right)_{\ell m \ell'm'} Y_{\ell m}\left(p\right)Y^*_{\ell' m'}\left(p'\right),
$$
where $Y_{\ell m}\left(p\right) = Y_{\ell m}\left(\hat{n}(p)\right)$ and $\hat{n}(p)$ are the spherical coordinates for pixel $p$, we can transform to spherical harmonics space,
\begin{align}
\nonumber
\left(N^{-1}\right)_{\ell m \ell' m'} &= \sum_{p p'}\left(N^{-1}\right)_{pp'}Y^{*}_{\ell m}(p)Y_{\ell' m'}(p')\\
\nonumber
&= \frac{N_{\mathrm{pix}}}{\sigma^2}\sum_{p p'} Y^{*}_{\ell m}(p)Y_{\ell' m'}(p') \delta_{pp'} H(|\theta -\pi/2|-b)\\
\nonumber
&= \frac{N_{\mathrm{pix}}}{\sigma^2}\sum_{p } Y^{*}_{\ell m}(p)Y_{\ell' m'}(p) H(|\theta -\pi/2|-b)\\
&= \frac{N_{\mathrm{pix}}}{\sigma^2}\sum_p \Tilde{Y}_{\ell m}\Tilde{Y}_{\ell' m'} e^{-i(m-m')\phi} H(|\theta -\pi/2|-b).
\end{align}
Here, we used that $Y_{\ell m}(p) = \tilde{Y}_{\ell m} e^{im\phi}$ where it is implied that $\tilde{Y}_{\ell m}=\tilde{Y}_{\ell m}(\theta)$, and $\theta$ and $\phi$ are functions of $p$.

We now take the limit where there is a large amount of pixels with equal surface area each. In this limit, we change the sum into an integration where we account for the number of pixels per area element
\begin{equation}
\sum_p \rightarrow \frac{N_{\mathrm{pix}}}{4\pi}\int d\Omega  = \frac{N_{\mathrm{pix}}}{4\pi}\int_{0}^{2\pi} d\phi \int_{0}^{\pi} d\theta \sin(\theta).
\end{equation}
This gives us in spherical coordinates
\begin{align}
\nonumber
\left(N^{-1}\right)_{\ell m \ell' m'} &= \frac{N_{\mathrm{pix}}^2}{4\pi \sigma^2}\int_{0}^{2\pi} d\phi \int_{0}^{\pi} d\theta \sin(\theta)\tilde{Y}_{\ell m}  \tilde{Y}_{\ell' m'}  e^{-i(m-m')\phi}
\\
\nonumber
&\cdot H(|\theta -\pi/2|-b)\\
\nonumber
&= \frac{N_{\mathrm{pix}}^2}{2\sigma^2} \delta_{mm'}\int_{0}^{\pi} d\theta \sin(\theta)\tilde{Y}_{\ell m}  \tilde{Y}_{\ell' m'} H(|\theta -\pi/2|-b)\\
\nonumber
&= \frac{N_{\mathrm{pix}}^2}{2\sigma^2} \delta_{mm'}\\
&\cdot \left(\int_{0}^{\pi/2-b} d\theta \sin(\theta) \tilde{Y}_{\ell m}  \tilde{Y}_{\ell' m'}+\int_{\pi/2+b}^{\pi} d\theta \sin(\theta)\tilde{Y}_{\ell m}  \tilde{Y}_{\ell' m'}\right).
\end{align}
Writing the above equation in terms of Legendre polynomials $P_{\ell m}(\cos(\theta)))$, we have
$\Tilde{Y}_{\ell m} = \Delta_{\ell m}P_{\ell m}(\cos(\theta)))$, where ${\Delta_{\ell m}=(-1)^m \sqrt{\frac{2\ell+1}{4\pi}\frac{(\ell - m)!}{(\ell+m)!}}}$.
 Writing $x=\cos(\theta)$, we know that Legendre polynomials $P_{\ell m}(x)$ are either symmetric or anti-symmetric in $x\rightarrow-x$. $P_{\ell m}(x)$ is symmetric in $x \rightarrow -x$ when $\ell+m$ = even and anti-symmetric when $\ell+m$ = odd. Since $m=m'$, we note that the two integrals cancel each other if $\ell+\ell' =$ odd. We, therefore, only get non-zero elements when $\ell + \ell' =$ even, for which the two integrals are equal. Hence, for $\ell + \ell'=$ even, we get
\begin{align}
\nonumber
\left(N^{-1}\right)_{\ell m \ell' m'} &= \frac{N_{\mathrm{pix}}^2}{\sigma^2} \delta_{mm'}\int_{0}^{\pi/2-b} d\theta \sin(\theta)\Tilde{Y}_{\ell m}(\theta)\Tilde{Y}_{\ell' m'}(\theta)\\
\label{eq:finished_n_inv}
&=\frac{N_{\mathrm{pix}}^2}{\sigma^2} \delta_{mm'}\int_{\sin(b)}^{1} dx \, \Tilde{Y}_{\ell m}(\arccos(x)) \Tilde{Y}_{\ell' m}(\arccos(x)).
\end{align}
This integral can be solved numerically by gridding $x$, and the spherical harmonics where the phase factor is removed can be calculated in Python using a library like ...

The azimuthally symmetric Galactic mask used for this work has a sky coverage of $f_{\mathrm{sky}} = 0.90$, giving $\sin(b) = 0.1$. Therefore, we require much fewer grid points for $x$ if we use the following identity for $\ell+\ell' = $ even:
\begin{align}
&\delta_{mm'} \int_{\sin(b)}^{1} dx \, \Tilde{Y}_{\ell m}(\arccos(x)) \Tilde{Y}_{\ell' m}(\arccos(x)) = \\
\frac{1}{4\pi}\delta_{\ell \ell'}\delta_{m m'} - &\delta_{mm'} \int_{0}^{\sin(b)} dx \, \Tilde{Y}_{\ell m}(\arccos(x)) \Tilde{Y}_{\ell' m}(\arccos(x)),
\end{align}
which comes from the orthonormality condition for spherical harmonics. We now only need to grid $x$ in the interval $0\leq x \leq \sin(b) = 0.1$, as opposed to the interval $0.9 \leq x \leq 1$.

Since $N^{-1}_{\ell m \ell' m'} \propto \delta_{m m'}$, we get simplified matrix expressions. Imagine multiplying the matrix $\boldsymbol{N}^{-1} = N^{-1}_{\ell m \ell' m'}$ with the vector $\boldsymbol{b} = b_{\ell m}$
\begin{align}
\boldsymbol{N}^{-1} \cdot \boldsymbol{b} &= \sum_{\ell' m'}\left(N^{-1}\right)_{\ell m \ell' m'}b_{\ell' m'} = \sum_{\ell'}\left(N^{-1}\right)_{\ell m \ell' m}b_{\ell' m}\\
&= \sum_{\ell' }\left(N^{-1}\right)^{(m)}_{\ell, \ell'}b^{(m)}_{\ell'}.
\end{align}
To solve the map-making equation, we get a matrix equation for each $0 \leq m \leq \ell_{\mathrm{max}}$. This gives us $\ell_{\textrm{max}}+1$ number of matrix equations where the dimensions of the matrices are maximally $\ell_{\textrm{max}} \times \ell_{\textrm{max}}$. This is numerically much quicker than inverting the full $(\ell_{\textrm{max}})^2 \times (\ell_{\textrm{max}})^2$ matrix once.


\end{document}
